{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b7e94c-7d42-41ea-a86b-9bcc476e8128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "from sedona.utils.adapter import Adapter\n",
    "from sedona.core.enums import GridType, IndexType\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "from sedona.sql import ST_IsValid, ST_MakeValid\n",
    "\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import col, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5869d5fb-0cc1-4961-a7a6-300b454c3d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.sedona#sedona-spark-shaded-3.0_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3637918f-3dcb-4189-a9a9-65c26a658698;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-spark-shaded-3.0_2.12;1.6.1 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.6.1-28.2 in central\n",
      ":: resolution report :: resolve 115ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.sedona#sedona-spark-shaded-3.0_2.12;1.6.1 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.6.1-28.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3637918f-3dcb-4189-a9a9-65c26a658698\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n",
      "24/10/05 17:05:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "config = SedonaContext.builder() .\\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.6.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.6.1-28.2'). \\\n",
    "    getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f624ed41-5695-4189-948a-dcb58662b232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "buildings_gdf = sedona.read.format(\"shapefile\").option(\"recursiveFileLookup\", \"true\").load(\"../ALKIS/*/GebauedeBauwerk.shp\").dropDuplicates([\"oid\"])\n",
    "nutzung_gdf = sedona.read.format(\"shapefile\").option(\"recursiveFileLookup\", \"true\").load(\"../ALKIS/*/NutzungFlurstueck.shp\").dropDuplicates([\"oid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78acd490-cdef-4d3f-8c8c-4240b531f5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2473571"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildings_gdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dfe2085-82ed-4cee-aa33-8e919490908a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5243004"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutzung_gdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d73d1fda-4c3a-4261-8c97-84b5da951bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- oid: string (nullable = true)\n",
      " |-- aktualit: string (nullable = true)\n",
      " |-- gebnutzbez: string (nullable = true)\n",
      " |-- funktion: string (nullable = true)\n",
      " |-- gfkzshh: string (nullable = true)\n",
      " |-- rellage: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- anzahlgs: long (nullable = true)\n",
      " |-- gmdschl: string (nullable = true)\n",
      " |-- lagebeztxt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buildings_gdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28dec44d-1f6f-4639-8b45-b4ca22f6f06d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- oid: string (nullable = true)\n",
      " |-- aktualit: string (nullable = true)\n",
      " |-- nutzart: string (nullable = true)\n",
      " |-- bez: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- flaeche: decimal(12,2) (nullable = true)\n",
      " |-- flstkennz: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nutzung_gdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3e054ba-9e05-4ae1-ad4b-865d2c8427b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buildings columns: 11\n",
      "Nutzung columns: 8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Buildings columns: {len(buildings_gdf.columns)}\")\n",
    "print(f\"Nutzung columns: {len(nutzung_gdf.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e012c9b3-8e8d-4dd5-a5ff-bcbc88d901cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:====================================================>   (17 + 1) / 18]\r"
     ]
    }
   ],
   "source": [
    "nutzung_RDD = Adapter.toSpatialRdd(nutzung_gdf, \"geometry\")\n",
    "buildings_RDD = Adapter.toSpatialRdd(buildings_gdf, \"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a32192f-2589-40c6-837d-27ed88a8363f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nutzung_RDD.analyze()\n",
    "buildings_RDD.analyze() \n",
    "\n",
    "nutzung_RDD.spatialPartitioning(GridType.KDBTREE)\n",
    "buildings_RDD.spatialPartitioning(nutzung_RDD.getPartitioner())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cdcb2a2-1149-49dc-bdd9-0999e7485534",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/05 17:11:37 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "[Stage 52:>                                                       (0 + 16) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during the spatial join: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 52.0 failed 1 times, most recent failure: Lost task 15.0 in stage 52.0 (TID 541) (28dd096dafc7 executor driver): org.locationtech.jts.geom.TopologyException: side location conflict [ (463724.13, 5796494.38, NaN) ]\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n",
      "\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n",
      "\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:227)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: org.locationtech.jts.geom.TopologyException: side location conflict [ (463724.13, 5796494.38, NaN) ]\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n",
      "\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n",
      "\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:227)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/05 17:11:47 ERROR Executor: Exception in task 15.0 in stage 52.0 (TID 541)\n",
      "org.locationtech.jts.geom.TopologyException: side location conflict [ (463724.13, 5796494.38, NaN) ]\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n",
      "\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n",
      "\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:227)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "24/10/05 17:11:47 WARN TaskSetManager: Lost task 15.0 in stage 52.0 (TID 541) (28dd096dafc7 executor driver): org.locationtech.jts.geom.TopologyException: side location conflict [ (463724.13, 5796494.38, NaN) ]\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n",
      "\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n",
      "\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:227)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "24/10/05 17:11:47 ERROR TaskSetManager: Task 15 in stage 52.0 failed 1 times; aborting job\n",
      "24/10/05 17:11:47 WARN TaskSetManager: Lost task 16.0 in stage 52.0 (TID 542) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:47 WARN TaskSetManager: Lost task 12.0 in stage 52.0 (TID 538) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:48 WARN TaskSetManager: Lost task 7.0 in stage 52.0 (TID 533) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):                                (0 + 13) / 32]\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:48 WARN TaskSetManager: Lost task 13.0 in stage 52.0 (TID 539) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):                                (0 + 12) / 32]\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:48 WARN TaskSetManager: Lost task 0.0 in stage 52.0 (TID 526) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):                                (0 + 11) / 32]\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:48 WARN TaskSetManager: Lost task 11.0 in stage 52.0 (TID 537) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):                                (0 + 10) / 32]\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:49 WARN TaskSetManager: Lost task 3.0 in stage 52.0 (TID 529) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:49 WARN TaskSetManager: Lost task 5.0 in stage 52.0 (TID 531) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:49 WARN TaskSetManager: Lost task 1.0 in stage 52.0 (TID 527) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:49 WARN TaskSetManager: Lost task 9.0 in stage 52.0 (TID 535) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:49 WARN TaskSetManager: Lost task 8.0 in stage 52.0 (TID 534) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:49 WARN TaskSetManager: Lost task 2.0 in stage 52.0 (TID 528) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:49 WARN TaskSetManager: Lost task 10.0 in stage 52.0 (TID 536) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):                                 (0 + 3) / 32]\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:49 WARN TaskSetManager: Lost task 14.0 in stage 52.0 (TID 540) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:49 WARN TaskSetManager: Lost task 6.0 in stage 52.0 (TID 532) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):                                 (0 + 1) / 32]\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:11:49 WARN TaskSetManager: Lost task 4.0 in stage 52.0 (TID 530) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "build_on_spatial_partitioned_rdd = True ## Set to TRUE only if run join query\n",
    "using_index = True\n",
    "\n",
    "try:\n",
    "    result_RDD = JoinQuery.SpatialJoinQueryFlat(buildings_RDD, nutzung_RDD, using_index, build_on_spatial_partitioned_rdd)\n",
    "    print(f\"Number of results: {result_RDD.count()}\")\n",
    "    print(result_RDD.take(5))  # Sample output\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the spatial join: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03ab8187-5d67-4480-b797-54c3c28250be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/05 17:06:34 WARN TaskSetManager: Lost task 12.0 in stage 31.0 (TID 418) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:06:34 WARN TaskSetManager: Lost task 15.0 in stage 31.0 (TID 421) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:06:34 WARN TaskSetManager: Lost task 7.0 in stage 31.0 (TID 413) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:06:34 WARN TaskSetManager: Lost task 4.0 in stage 31.0 (TID 410) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):                                 (0 + 2) / 30]\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:06:34 WARN TaskSetManager: Lost task 2.0 in stage 31.0 (TID 408) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "24/10/05 17:06:34 WARN TaskSetManager: Lost task 13.0 in stage 31.0 (TID 419) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result columns: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_gdf = Adapter.toDf(result_RDD, sedona)\n",
    "print(f\"Result columns: {len(result_gdf.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf1a627c-1a6a-4c64-997e-ac15b0bfe419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: geometry (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: string (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: string (nullable = true)\n",
      " |-- _7: string (nullable = true)\n",
      " |-- _8: string (nullable = true)\n",
      " |-- _9: geometry (nullable = true)\n",
      " |-- _10: string (nullable = true)\n",
      " |-- _11: string (nullable = true)\n",
      " |-- _12: string (nullable = true)\n",
      " |-- _13: string (nullable = true)\n",
      " |-- _14: string (nullable = true)\n",
      " |-- _15: string (nullable = true)\n",
      " |-- _16: string (nullable = true)\n",
      " |-- _17: string (nullable = true)\n",
      " |-- _18: string (nullable = true)\n",
      " |-- _19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_gdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5daafb28-8b3f-4393-ad17-16ed643a876a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nutzung_oid: string (nullable = true)\n",
      " |-- nutzung_aktualit: string (nullable = true)\n",
      " |-- nutzung_nutzart: string (nullable = true)\n",
      " |-- nutzung_bez: string (nullable = true)\n",
      " |-- nutzung_name: string (nullable = true)\n",
      " |-- nutzung_flaeche: string (nullable = true)\n",
      " |-- nutzung_flstkennz: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- buildings_oid: string (nullable = true)\n",
      " |-- buildings_aktualit: string (nullable = true)\n",
      " |-- buildings_gebnutzbez: string (nullable = true)\n",
      " |-- buildings_funktion: string (nullable = true)\n",
      " |-- buildings_gfkzshh: string (nullable = true)\n",
      " |-- buildings_rellage: string (nullable = true)\n",
      " |-- buildings_name: string (nullable = true)\n",
      " |-- buildings_anzahlgs: string (nullable = true)\n",
      " |-- buildings_gmdschl: string (nullable = true)\n",
      " |-- buildings_lagebeztxt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add prefixes to distinguish between geometries, if needed\n",
    "combined_columns = [\"nutzung_\" + col for col in nutzung_gdf.columns] + [\"buildings_\" + col for col in buildings_gdf.columns]\n",
    "\n",
    "# Rename columns in result_gdf\n",
    "for i, col_name in enumerate(combined_columns):\n",
    "    result_gdf = result_gdf.withColumnRenamed(f\"_{i + 1}\", col_name)\n",
    "\n",
    "result_gdf = result_gdf.drop(\"nutzung_geometry\").withColumnRenamed(\"buildings_geometry\", \"geometry\")\n",
    "result_gdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a274c7e3-f20c-4b49-a2da-80506166f061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@udf(returnType=BooleanType())\n",
    "def is_geometry_valid(wkt):\n",
    "    try:\n",
    "        from shapely import wkt as shapely_wkt\n",
    "        from shape.errors import ShapelyError\n",
    "        \n",
    "        geom =  shapely_wkt.loads(wkt)\n",
    "        return geom.is_valid\n",
    "    except ShapelyError:\n",
    "        return false\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d327d3f3-af6d-4c8a-b7d6-623f923b3078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/05 17:06:35 ERROR Executor: Exception in task 14.0 in stage 41.0 (TID 438)\n",
      "org.locationtech.jts.geom.TopologyException: side location conflict [ (274565.7199458984, 5882096.033370846, NaN) ]\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n",
      "\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n",
      "\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:227)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "24/10/05 17:06:35 WARN TaskSetManager: Lost task 14.0 in stage 41.0 (TID 438) (28dd096dafc7 executor driver): org.locationtech.jts.geom.TopologyException: side location conflict [ (274565.7199458984, 5882096.033370846, NaN) ]\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n",
      "\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n",
      "\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:227)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "24/10/05 17:06:35 ERROR TaskSetManager: Task 14 in stage 41.0 failed 1 times; aborting job\n",
      "24/10/05 17:06:35 WARN TaskSetManager: Lost task 15.0 in stage 41.0 (TID 439) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:35 WARN TaskSetManager: Lost task 12.0 in stage 41.0 (TID 436) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:35 WARN TaskSetManager: Lost task 0.0 in stage 41.0 (TID 424) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:35 WARN TaskSetManager: Lost task 16.0 in stage 41.0 (TID 440) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:35 WARN TaskSetManager: Lost task 6.0 in stage 41.0 (TID 430) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:35 WARN TaskSetManager: Lost task 8.0 in stage 41.0 (TID 432) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:35 WARN TaskSetManager: Lost task 3.0 in stage 41.0 (TID 427) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:35 WARN TaskSetManager: Lost task 4.0 in stage 41.0 (TID 428) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:35 WARN TaskSetManager: Lost task 5.0 in stage 41.0 (TID 429) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:35 WARN TaskSetManager: Lost task 1.0 in stage 41.0 (TID 425) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:35 WARN TaskSetManager: Lost task 10.0 in stage 41.0 (TID 434) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:35 WARN TaskSetManager: Lost task 7.0 in stage 41.0 (TID 431) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:36 WARN TaskSetManager: Lost task 2.0 in stage 41.0 (TID 426) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:36 WARN TaskSetManager: Lost task 9.0 in stage 41.0 (TID 433) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:36 WARN TaskSetManager: Lost task 11.0 in stage 41.0 (TID 435) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/05 17:06:36 WARN TaskSetManager: Lost task 13.0 in stage 41.0 (TID 437) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o146.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14 in stage 41.0 failed 1 times, most recent failure: Lost task 14.0 in stage 41.0 (TID 438) (28dd096dafc7 executor driver): org.locationtech.jts.geom.TopologyException: side location conflict [ (274565.7199458984, 5882096.033370846, NaN) ]\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:125)\n\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:227)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.locationtech.jts.geom.TopologyException: side location conflict [ (274565.7199458984, 5882096.033370846, NaN) ]\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:125)\n\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:227)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m valid_gdf \u001b[38;5;241m=\u001b[39m result_gdf\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_geometry\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_geometry_valid(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m----> 2\u001b[0m total_validity \u001b[38;5;241m=\u001b[39m \u001b[43mvalid_gdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeometry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o146.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14 in stage 41.0 failed 1 times, most recent failure: Lost task 14.0 in stage 41.0 (TID 438) (28dd096dafc7 executor driver): org.locationtech.jts.geom.TopologyException: side location conflict [ (274565.7199458984, 5882096.033370846, NaN) ]\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:125)\n\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:227)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.locationtech.jts.geom.TopologyException: side location conflict [ (274565.7199458984, 5882096.033370846, NaN) ]\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:125)\n\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:227)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n"
     ]
    }
   ],
   "source": [
    "valid_gdf = result_gdf.withColumn(\"is_geometry\", is_geometry_valid(col(\"geometry\")))\n",
    "total_validity = valid_gdf.drop(\"geometry\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098113b8-9ae4-4804-8068-6801e44dd309",
   "metadata": {},
   "outputs": [],
   "source": [
    "(valid_gdf.write\n",
    " .format(\"geoparquet\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"geoparquet.version\", \"1.0.0\")\n",
    " .save(\"sedona_data.geoparquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e713cc7-0aed-414c-a3d8-b5e5b74bfb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aadc4b6-e52c-4bc8-aca3-d92c335d5b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
