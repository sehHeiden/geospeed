{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b7e94c-7d42-41ea-a86b-9bcc476e8128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sedona.spark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5869d5fb-0cc1-4961-a7a6-300b454c3d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.sedona#sedona-spark-shaded-3.0_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-020d62a0-9866-4dcf-83a5-e34529d45d1a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-spark-shaded-3.0_2.12;1.6.1 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.6.1-28.2 in central\n",
      ":: resolution report :: resolve 126ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.sedona#sedona-spark-shaded-3.0_2.12;1.6.1 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.6.1-28.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-020d62a0-9866-4dcf-83a5-e34529d45d1a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "24/10/07 18:10:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "config = SedonaContext.builder() .\\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.6.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.6.1-28.2'). \\\n",
    "    getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f624ed41-5695-4189-948a-dcb58662b232",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "buildings_gdf = sedona.read.format(\"shapefile\").option(\"recursiveFileLookup\", \"true\").load(\"../ALKIS/*/GebauedeBauwerk.shp\").dropDuplicates([\"oid\"])\n",
    "buildings_gdf.createOrReplaceTempView(\"buildings\")\n",
    "nutzung_gdf = sedona.read.format(\"shapefile\").option(\"recursiveFileLookup\", \"true\").load(\"../ALKIS/*/NutzungFlurstueck.shp\").dropDuplicates([\"oid\"])\n",
    "nutzung_gdf.createOrReplaceTempView(\"usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78acd490-cdef-4d3f-8c8c-4240b531f5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2473571"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildings_gdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dfe2085-82ed-4cee-aa33-8e919490908a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5243004"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutzung_gdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d73d1fda-4c3a-4261-8c97-84b5da951bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- oid: string (nullable = true)\n",
      " |-- aktualit: string (nullable = true)\n",
      " |-- gebnutzbez: string (nullable = true)\n",
      " |-- funktion: string (nullable = true)\n",
      " |-- gfkzshh: string (nullable = true)\n",
      " |-- rellage: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- anzahlgs: long (nullable = true)\n",
      " |-- gmdschl: string (nullable = true)\n",
      " |-- lagebeztxt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buildings_gdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28dec44d-1f6f-4639-8b45-b4ca22f6f06d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- oid: string (nullable = true)\n",
      " |-- aktualit: string (nullable = true)\n",
      " |-- nutzart: string (nullable = true)\n",
      " |-- bez: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- flaeche: decimal(12,2) (nullable = true)\n",
      " |-- flstkennz: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nutzung_gdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc83fe0e-51cc-47af-a490-c361e5b963ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_gdf = sedona.sql(\"\"\"SELECT ST_Intersection(b.geometry, u.geometry) AS geometry, b.oid AS building_oid, b.gebnutzbez, b.gfkzshh, b.name, b.anzahlgs, b.gmdschl, b.lagebeztxt, b.funktion, u.oid AS flur_oid, u.nutzart, u.bez, u.flstkennz\n",
    "                       FROM buildings b, usage u \n",
    "                       WHERE ST_Intersects(b.geometry, u.geometry)\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e96e50a-5c7f-4086-b848-ba0a1ce864ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- building_oid: string (nullable = true)\n",
      " |-- gebnutzbez: string (nullable = true)\n",
      " |-- gfkzshh: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- anzahlgs: long (nullable = true)\n",
      " |-- gmdschl: string (nullable = true)\n",
      " |-- lagebeztxt: string (nullable = true)\n",
      " |-- funktion: string (nullable = true)\n",
      " |-- flur_oid: string (nullable = true)\n",
      " |-- nutzart: string (nullable = true)\n",
      " |-- bez: string (nullable = true)\n",
      " |-- flstkennz: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_gdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "789847dd-79ba-4fe7-8f83-e12e549d0d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_gdf.createOrReplaceTempView(\"result\")\n",
    "\n",
    "result2_gdf = sedona.sql(\"\"\"\n",
    "                        SELECT ST_MakeValid(geometry), building_oid, gebnutzbez, gfkzshh, name, anzahlgs, gmdschl, lagebeztxt, funktion, flur_oid, nutzart, bez, flstkennz\n",
    "                        FROM result\n",
    "                         \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d2060990-011d-4602-87ae-08467c24cb6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/07 20:01:34 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "[Stage 282:==================================================>    (21 + 2) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----------+----------+--------------------+--------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            geometry|      building_oid|gebnutzbez|   gfkzshh|                name|anzahlgs| gmdschl|          lagebeztxt|            funktion|            flur_oid|             nutzart|                 bez|           flstkennz|\n",
      "+--------------------+------------------+----------+----------+--------------------+--------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|POLYGON ((335085....|DEBBAL01000coyoaBL|  Gebaeude|31001_2463|              Garage|       1|12069020|                    |              Garage|DEBBAL010007Eu5HD...|     Strassenverkehr|                    |12156700600255______|\n",
      "|POLYGON ((322476....|DEBBAL6900027Sq8BL|  Gebaeude|31001_1000|         Wohngebäude|    null|12069665|                    |         Wohngebäude|DEBBAL010007QD0JD...|Industrie Und Gew...|Industrie und Gew...|121569004000260003__|\n",
      "|POLYGON ((322495....|DEBBAL6900027Sq9BL|  Gebaeude|31001_1000|         Wohngebäude|    null|12069665|        Alte Hölle 1|         Wohngebäude|DEBBAL010007QD0JD...|Industrie Und Gew...|Industrie und Gew...|121569004000260003__|\n",
      "|POLYGON ((322448....|DEBBAL6900027Sq7BL|  Gebaeude|31001_2081|Gaststätte, Resta...|    null|12069665|                    |Gaststätte, Resta...|DEBBAL010007QD0JD...|Industrie Und Gew...|Industrie und Gew...|121569004000260003__|\n",
      "|POLYGON ((322487....|DEBBAL6900027SqaBL|  Gebaeude|31001_2081|Gaststätte, Resta...|    null|12069665|                    |Gaststätte, Resta...|DEBBAL010007QD0JD...|Industrie Und Gew...|Industrie und Gew...|121569004000260003__|\n",
      "|POLYGON ((334801....|DEBBAL690000gNw9BL|  Gebaeude|31001_2721|             Scheune|    null|12069020|                    |             Scheune|DEBBAL010007Szz4D...|      Wohnbauflaeche|                    |121503012002000001__|\n",
      "|POLYGON ((334804....|DEBBAL690000gNwcBL|  Gebaeude|31001_2721|             Scheune|    null|12069020|                    |             Scheune|DEBBAL010007Szz4D...|      Wohnbauflaeche|                    |12150301200201______|\n",
      "|POLYGON ((336282....|DEBBAL690000gMaXBL|  Gebaeude|31001_9998|Nach Quellenlage ...|    null|12069020|                    |Nach Quellenlage ...|DEBBAL010007VVgdD...|      Wohnbauflaeche|                    |121503007001950002__|\n",
      "|POLYGON ((336278....|DEBBAL690000gMb1BL|  Gebaeude|31001_1010|            Wohnhaus|    null|12069020|Brücker Landstraß...|            Wohnhaus|DEBBAL010007VVgdD...|      Wohnbauflaeche|                    |121503007001950002__|\n",
      "|POLYGON ((336269....|DEBBAL690000gMaQBL|  Gebaeude|31001_2463|              Garage|    null|12069020|                    |              Garage|DEBBAL010007VVgdD...|      Wohnbauflaeche|                    |121503007001950002__|\n",
      "|POLYGON ((331082....|DEBBAL690000ofkrBL|  Gebaeude|31001_9998|Nach Quellenlage ...|    null|12069020|                    |Nach Quellenlage ...|DEBBAL010007XKQFD...|      Wohnbauflaeche|                    |12151100200061______|\n",
      "|POLYGON ((319047....|DEBBAL690006XsRlBL|  Gebaeude|31001_2120|           Werkstatt|    null|12069232|                    |           Werkstatt|DEBBAL010007nJe7D...|      Wohnbauflaeche|                    |12176300400010______|\n",
      "|POLYGON ((319047....|DEBBAL690006XsRiBL|  Gebaeude|31001_2463|              Garage|    null|12069232|                    |              Garage|DEBBAL010007nJe7D...|      Wohnbauflaeche|                    |12176300400010______|\n",
      "|POLYGON ((319029....|DEBBAL690006XsR1BL|  Gebaeude|31001_1010|            Wohnhaus|    null|12069232|       Dorfstraße 35|            Wohnhaus|DEBBAL010007nJe7D...|      Wohnbauflaeche|                    |12176300400010______|\n",
      "|POLYGON ((319029....|DEBBAL690006XsQNBL|  Gebaeude|31001_9998|Nach Quellenlage ...|    null|12069232|                    |Nach Quellenlage ...|DEBBAL010007nJe7D...|      Wohnbauflaeche|                    |12176300400010______|\n",
      "|POLYGON ((319018....|DEBBAL690006XsQGBL|  Gebaeude|31001_2720|Land- und forstwi...|    null|12069232|                    |Land- und forstwi...|DEBBAL010007nJe7D...|      Wohnbauflaeche|                    |12176300400010______|\n",
      "|POLYGON ((319031....|DEBBAL690006XsR2BL|  Gebaeude|31001_2724|               Stall|    null|12069232|                    |               Stall|DEBBAL010007nJe7D...|      Wohnbauflaeche|                    |12176300400010______|\n",
      "|POLYGON ((344767....|DEBBAL0100087HcrBL|  Gebaeude|31001_1010|            Wohnhaus|       2|12069306|        Mittelweg 16|            Wohnhaus|DEBBAL010007zfSfD...|      Wohnbauflaeche|                    |12175000200645______|\n",
      "|POLYGON ((321860....|DEBBAL6900027SilBL|  Gebaeude|31001_9998|Nach Quellenlage ...|    null|12069665|                    |Nach Quellenlage ...|DEBBAL01000819YVD...|      Landwirtschaft|           Brachland|12156900100132______|\n",
      "|POLYGON ((321869....|DEBBAL6900027SizBL|  Gebaeude|31001_9998|Nach Quellenlage ...|    null|12069665|                    |Nach Quellenlage ...|DEBBAL01000819YVD...|      Landwirtschaft|           Brachland|12156900100132______|\n",
      "+--------------------+------------------+----------+----------+--------------------+--------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result2_gdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "75e8e8f5-cbb7-4e90-a095-81b84ae89456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/07 20:02:01 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "24/10/07 20:02:05 ERROR Executor: Exception in task 0.0 in stage 294.0 (TID 3518)\n",
      "org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression ST_Intersection - inputs: [POLYGON ((315056.626 5793716.89, 315053.766 5793719.782, 315058.583 5793724.51, 315058.044 5793725.017, 315061.31 5793728.339, 315061.411 5793728.25, 315063.105 5793729.971, 315060.495 5793732.497, 315061.296 5793733.282, 315056.48 5793738.006, 315063.126 5793744.858, 315073.89 5793734.467, 315066.974 5793727.383, 315064.59 5793725.022, 315061.447 5793721.812, 315060.804 5793721.166, 315056.626 5793716.89)), MULTIPOLYGON (((315050.07 5793723.45, 315056.63 5793716.89, 315053.75 5793719.77, 315050.07 5793723.45)), ((315050.07 5793723.45, 315053.75 5793719.77, 315056.63 5793716.89, 315050.52 5793714.47, 315045.44 5793708.28, 315031.32 5793688.73, 315024.03 5793672.83, 315015.58 5793655.35, 315000.21 5793624.54, 314993.26 5793610.15, 314988.86 5793601.52, 314985.81 5793595.53, 314979.26 5793598.82, 314969.12 5793602.4, 314960.37 5793605.91, 314955.01 5793608.05, 314949.88 5793609.89, 314944.57 5793609.55, 314939.79 5793609.23, 314933 5793606.27, 314928.72 5793604.86, 314916.03 5793616.9, 314903.12 5793629.64, 314925.5 5793653.58, 315006.84 5793720.15, 315031.7 5793738.82, 315034.07 5793740.61, 315050.35 5793755.49, 315059.64 5793747.79, 315042.87 5793730.47, 315050.07 5793723.45)))], cause: no outgoing dirEdge found [ (315056.2685, 5793717.251499999, NaN) ]\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:127)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.implicits$InputExpressionEnhancer.toGeometry(implicits.scala:35)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredTypes$.$anonfun$buildArgumentExtractor$2(InferredExpression.scala:202)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.$anonfun$buildExtractors$2(InferredExpression.scala:97)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction1$2(InferrableFunctionConverter.scala:37)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n",
      "\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5(TraitJoinQueryExec.scala:140)\n",
      "\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5$adapted(TraitJoinQueryExec.scala:140)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: org.locationtech.jts.geom.TopologyException: no outgoing dirEdge found [ (315056.2685, 5793717.251499999, NaN) ]\n",
      "\tat org.locationtech.jts.geomgraph.DirectedEdgeStar.linkResultDirectedEdges(DirectedEdgeStar.java:225)\n",
      "\tat org.locationtech.jts.geomgraph.PlanarGraph.linkResultDirectedEdges(PlanarGraph.java:61)\n",
      "\tat org.locationtech.jts.operation.overlay.PolygonBuilder.add(PolygonBuilder.java:66)\n",
      "\tat org.locationtech.jts.operation.overlay.PolygonBuilder.add(PolygonBuilder.java:56)\n",
      "\tat org.locationtech.jts.operation.overlay.OverlayOp.computeOverlay(OverlayOp.java:248)\n",
      "\tat org.locationtech.jts.operation.overlay.OverlayOp.getResultGeometry(OverlayOp.java:181)\n",
      "\tat org.locationtech.jts.operation.overlay.OverlayOp.overlayOp(OverlayOp.java:84)\n",
      "\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.getResultGeometry(SnapIfNeededOverlayOp.java:75)\n",
      "\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.overlayOp(SnapIfNeededOverlayOp.java:37)\n",
      "\tat org.locationtech.jts.geom.GeometryOverlay.overlay(GeometryOverlay.java:76)\n",
      "\tat org.locationtech.jts.geom.GeometryOverlay.intersection(GeometryOverlay.java:119)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersection(Geometry.java:1330)\n",
      "\tat org.apache.sedona.common.Functions.intersection(Functions.java:987)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:58)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:121)\n",
      "\t... 26 more\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 0.0 in stage 294.0 (TID 3518) (28dd096dafc7 executor driver): org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression ST_Intersection - inputs: [POLYGON ((315056.626 5793716.89, 315053.766 5793719.782, 315058.583 5793724.51, 315058.044 5793725.017, 315061.31 5793728.339, 315061.411 5793728.25, 315063.105 5793729.971, 315060.495 5793732.497, 315061.296 5793733.282, 315056.48 5793738.006, 315063.126 5793744.858, 315073.89 5793734.467, 315066.974 5793727.383, 315064.59 5793725.022, 315061.447 5793721.812, 315060.804 5793721.166, 315056.626 5793716.89)), MULTIPOLYGON (((315050.07 5793723.45, 315056.63 5793716.89, 315053.75 5793719.77, 315050.07 5793723.45)), ((315050.07 5793723.45, 315053.75 5793719.77, 315056.63 5793716.89, 315050.52 5793714.47, 315045.44 5793708.28, 315031.32 5793688.73, 315024.03 5793672.83, 315015.58 5793655.35, 315000.21 5793624.54, 314993.26 5793610.15, 314988.86 5793601.52, 314985.81 5793595.53, 314979.26 5793598.82, 314969.12 5793602.4, 314960.37 5793605.91, 314955.01 5793608.05, 314949.88 5793609.89, 314944.57 5793609.55, 314939.79 5793609.23, 314933 5793606.27, 314928.72 5793604.86, 314916.03 5793616.9, 314903.12 5793629.64, 314925.5 5793653.58, 315006.84 5793720.15, 315031.7 5793738.82, 315034.07 5793740.61, 315050.35 5793755.49, 315059.64 5793747.79, 315042.87 5793730.47, 315050.07 5793723.45)))], cause: no outgoing dirEdge found [ (315056.2685, 5793717.251499999, NaN) ]\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:127)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.implicits$InputExpressionEnhancer.toGeometry(implicits.scala:35)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredTypes$.$anonfun$buildArgumentExtractor$2(InferredExpression.scala:202)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.$anonfun$buildExtractors$2(InferredExpression.scala:97)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction1$2(InferrableFunctionConverter.scala:37)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n",
      "\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5(TraitJoinQueryExec.scala:140)\n",
      "\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5$adapted(TraitJoinQueryExec.scala:140)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: org.locationtech.jts.geom.TopologyException: no outgoing dirEdge found [ (315056.2685, 5793717.251499999, NaN) ]\n",
      "\tat org.locationtech.jts.geomgraph.DirectedEdgeStar.linkResultDirectedEdges(DirectedEdgeStar.java:225)\n",
      "\tat org.locationtech.jts.geomgraph.PlanarGraph.linkResultDirectedEdges(PlanarGraph.java:61)\n",
      "\tat org.locationtech.jts.operation.overlay.PolygonBuilder.add(PolygonBuilder.java:66)\n",
      "\tat org.locationtech.jts.operation.overlay.PolygonBuilder.add(PolygonBuilder.java:56)\n",
      "\tat org.locationtech.jts.operation.overlay.OverlayOp.computeOverlay(OverlayOp.java:248)\n",
      "\tat org.locationtech.jts.operation.overlay.OverlayOp.getResultGeometry(OverlayOp.java:181)\n",
      "\tat org.locationtech.jts.operation.overlay.OverlayOp.overlayOp(OverlayOp.java:84)\n",
      "\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.getResultGeometry(SnapIfNeededOverlayOp.java:75)\n",
      "\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.overlayOp(SnapIfNeededOverlayOp.java:37)\n",
      "\tat org.locationtech.jts.geom.GeometryOverlay.overlay(GeometryOverlay.java:76)\n",
      "\tat org.locationtech.jts.geom.GeometryOverlay.intersection(GeometryOverlay.java:119)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersection(Geometry.java:1330)\n",
      "\tat org.apache.sedona.common.Functions.intersection(Functions.java:987)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:58)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:121)\n",
      "\t... 26 more\n",
      "\n",
      "24/10/07 20:02:05 ERROR TaskSetManager: Task 0 in stage 294.0 failed 1 times; aborting job\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 6.0 in stage 294.0 (TID 3524) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 12.0 in stage 294.0 (TID 3530) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 8.0 in stage 294.0 (TID 3526) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 13.0 in stage 294.0 (TID 3531) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 14.0 in stage 294.0 (TID 3532) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 1.0 in stage 294.0 (TID 3519) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 9.0 in stage 294.0 (TID 3527) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 15.0 in stage 294.0 (TID 3533) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 10.0 in stage 294.0 (TID 3528) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 2.0 in stage 294.0 (TID 3520) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 11.0 in stage 294.0 (TID 3529) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 16.0 in stage 294.0 (TID 3534) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 5.0 in stage 294.0 (TID 3523) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 4.0 in stage 294.0 (TID 3522) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 7.0 in stage 294.0 (TID 3525) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 20:02:05 WARN TaskSetManager: Lost task 3.0 in stage 294.0 (TID 3521) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o255.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 294.0 failed 1 times, most recent failure: Lost task 0.0 in stage 294.0 (TID 3518) (28dd096dafc7 executor driver): org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression ST_Intersection - inputs: [POLYGON ((315056.626 5793716.89, 315053.766 5793719.782, 315058.583 5793724.51, 315058.044 5793725.017, 315061.31 5793728.339, 315061.411 5793728.25, 315063.105 5793729.971, 315060.495 5793732.497, 315061.296 5793733.282, 315056.48 5793738.006, 315063.126 5793744.858, 315073.89 5793734.467, 315066.974 5793727.383, 315064.59 5793725.022, 315061.447 5793721.812, 315060.804 5793721.166, 315056.626 5793716.89)), MULTIPOLYGON (((315050.07 5793723.45, 315056.63 5793716.89, 315053.75 5793719.77, 315050.07 5793723.45)), ((315050.07 5793723.45, 315053.75 5793719.77, 315056.63 5793716.89, 315050.52 5793714.47, 315045.44 5793708.28, 315031.32 5793688.73, 315024.03 5793672.83, 315015.58 5793655.35, 315000.21 5793624.54, 314993.26 5793610.15, 314988.86 5793601.52, 314985.81 5793595.53, 314979.26 5793598.82, 314969.12 5793602.4, 314960.37 5793605.91, 314955.01 5793608.05, 314949.88 5793609.89, 314944.57 5793609.55, 314939.79 5793609.23, 314933 5793606.27, 314928.72 5793604.86, 314916.03 5793616.9, 314903.12 5793629.64, 314925.5 5793653.58, 315006.84 5793720.15, 315031.7 5793738.82, 315034.07 5793740.61, 315050.35 5793755.49, 315059.64 5793747.79, 315042.87 5793730.47, 315050.07 5793723.45)))], cause: no outgoing dirEdge found [ (315056.2685, 5793717.251499999, NaN) ]\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:127)\n\tat org.apache.spark.sql.sedona_sql.expressions.implicits$InputExpressionEnhancer.toGeometry(implicits.scala:35)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredTypes$.$anonfun$buildArgumentExtractor$2(InferredExpression.scala:202)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.$anonfun$buildExtractors$2(InferredExpression.scala:97)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction1$2(InferrableFunctionConverter.scala:37)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5(TraitJoinQueryExec.scala:140)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5$adapted(TraitJoinQueryExec.scala:140)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.locationtech.jts.geom.TopologyException: no outgoing dirEdge found [ (315056.2685, 5793717.251499999, NaN) ]\n\tat org.locationtech.jts.geomgraph.DirectedEdgeStar.linkResultDirectedEdges(DirectedEdgeStar.java:225)\n\tat org.locationtech.jts.geomgraph.PlanarGraph.linkResultDirectedEdges(PlanarGraph.java:61)\n\tat org.locationtech.jts.operation.overlay.PolygonBuilder.add(PolygonBuilder.java:66)\n\tat org.locationtech.jts.operation.overlay.PolygonBuilder.add(PolygonBuilder.java:56)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.computeOverlay(OverlayOp.java:248)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.getResultGeometry(OverlayOp.java:181)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.overlayOp(OverlayOp.java:84)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.getResultGeometry(SnapIfNeededOverlayOp.java:75)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.overlayOp(SnapIfNeededOverlayOp.java:37)\n\tat org.locationtech.jts.geom.GeometryOverlay.overlay(GeometryOverlay.java:76)\n\tat org.locationtech.jts.geom.GeometryOverlay.intersection(GeometryOverlay.java:119)\n\tat org.locationtech.jts.geom.Geometry.intersection(Geometry.java:1330)\n\tat org.apache.sedona.common.Functions.intersection(Functions.java:987)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:58)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:121)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression ST_Intersection - inputs: [POLYGON ((315056.626 5793716.89, 315053.766 5793719.782, 315058.583 5793724.51, 315058.044 5793725.017, 315061.31 5793728.339, 315061.411 5793728.25, 315063.105 5793729.971, 315060.495 5793732.497, 315061.296 5793733.282, 315056.48 5793738.006, 315063.126 5793744.858, 315073.89 5793734.467, 315066.974 5793727.383, 315064.59 5793725.022, 315061.447 5793721.812, 315060.804 5793721.166, 315056.626 5793716.89)), MULTIPOLYGON (((315050.07 5793723.45, 315056.63 5793716.89, 315053.75 5793719.77, 315050.07 5793723.45)), ((315050.07 5793723.45, 315053.75 5793719.77, 315056.63 5793716.89, 315050.52 5793714.47, 315045.44 5793708.28, 315031.32 5793688.73, 315024.03 5793672.83, 315015.58 5793655.35, 315000.21 5793624.54, 314993.26 5793610.15, 314988.86 5793601.52, 314985.81 5793595.53, 314979.26 5793598.82, 314969.12 5793602.4, 314960.37 5793605.91, 314955.01 5793608.05, 314949.88 5793609.89, 314944.57 5793609.55, 314939.79 5793609.23, 314933 5793606.27, 314928.72 5793604.86, 314916.03 5793616.9, 314903.12 5793629.64, 314925.5 5793653.58, 315006.84 5793720.15, 315031.7 5793738.82, 315034.07 5793740.61, 315050.35 5793755.49, 315059.64 5793747.79, 315042.87 5793730.47, 315050.07 5793723.45)))], cause: no outgoing dirEdge found [ (315056.2685, 5793717.251499999, NaN) ]\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:127)\n\tat org.apache.spark.sql.sedona_sql.expressions.implicits$InputExpressionEnhancer.toGeometry(implicits.scala:35)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredTypes$.$anonfun$buildArgumentExtractor$2(InferredExpression.scala:202)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.$anonfun$buildExtractors$2(InferredExpression.scala:97)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction1$2(InferrableFunctionConverter.scala:37)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5(TraitJoinQueryExec.scala:140)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5$adapted(TraitJoinQueryExec.scala:140)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.locationtech.jts.geom.TopologyException: no outgoing dirEdge found [ (315056.2685, 5793717.251499999, NaN) ]\n\tat org.locationtech.jts.geomgraph.DirectedEdgeStar.linkResultDirectedEdges(DirectedEdgeStar.java:225)\n\tat org.locationtech.jts.geomgraph.PlanarGraph.linkResultDirectedEdges(PlanarGraph.java:61)\n\tat org.locationtech.jts.operation.overlay.PolygonBuilder.add(PolygonBuilder.java:66)\n\tat org.locationtech.jts.operation.overlay.PolygonBuilder.add(PolygonBuilder.java:56)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.computeOverlay(OverlayOp.java:248)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.getResultGeometry(OverlayOp.java:181)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.overlayOp(OverlayOp.java:84)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.getResultGeometry(SnapIfNeededOverlayOp.java:75)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.overlayOp(SnapIfNeededOverlayOp.java:37)\n\tat org.locationtech.jts.geom.GeometryOverlay.overlay(GeometryOverlay.java:76)\n\tat org.locationtech.jts.geom.GeometryOverlay.intersection(GeometryOverlay.java:119)\n\tat org.locationtech.jts.geom.Geometry.intersection(Geometry.java:1330)\n\tat org.apache.sedona.common.Functions.intersection(Functions.java:987)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:58)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:121)\n\t... 26 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult2_gdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o255.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 294.0 failed 1 times, most recent failure: Lost task 0.0 in stage 294.0 (TID 3518) (28dd096dafc7 executor driver): org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression ST_Intersection - inputs: [POLYGON ((315056.626 5793716.89, 315053.766 5793719.782, 315058.583 5793724.51, 315058.044 5793725.017, 315061.31 5793728.339, 315061.411 5793728.25, 315063.105 5793729.971, 315060.495 5793732.497, 315061.296 5793733.282, 315056.48 5793738.006, 315063.126 5793744.858, 315073.89 5793734.467, 315066.974 5793727.383, 315064.59 5793725.022, 315061.447 5793721.812, 315060.804 5793721.166, 315056.626 5793716.89)), MULTIPOLYGON (((315050.07 5793723.45, 315056.63 5793716.89, 315053.75 5793719.77, 315050.07 5793723.45)), ((315050.07 5793723.45, 315053.75 5793719.77, 315056.63 5793716.89, 315050.52 5793714.47, 315045.44 5793708.28, 315031.32 5793688.73, 315024.03 5793672.83, 315015.58 5793655.35, 315000.21 5793624.54, 314993.26 5793610.15, 314988.86 5793601.52, 314985.81 5793595.53, 314979.26 5793598.82, 314969.12 5793602.4, 314960.37 5793605.91, 314955.01 5793608.05, 314949.88 5793609.89, 314944.57 5793609.55, 314939.79 5793609.23, 314933 5793606.27, 314928.72 5793604.86, 314916.03 5793616.9, 314903.12 5793629.64, 314925.5 5793653.58, 315006.84 5793720.15, 315031.7 5793738.82, 315034.07 5793740.61, 315050.35 5793755.49, 315059.64 5793747.79, 315042.87 5793730.47, 315050.07 5793723.45)))], cause: no outgoing dirEdge found [ (315056.2685, 5793717.251499999, NaN) ]\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:127)\n\tat org.apache.spark.sql.sedona_sql.expressions.implicits$InputExpressionEnhancer.toGeometry(implicits.scala:35)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredTypes$.$anonfun$buildArgumentExtractor$2(InferredExpression.scala:202)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.$anonfun$buildExtractors$2(InferredExpression.scala:97)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction1$2(InferrableFunctionConverter.scala:37)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5(TraitJoinQueryExec.scala:140)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5$adapted(TraitJoinQueryExec.scala:140)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.locationtech.jts.geom.TopologyException: no outgoing dirEdge found [ (315056.2685, 5793717.251499999, NaN) ]\n\tat org.locationtech.jts.geomgraph.DirectedEdgeStar.linkResultDirectedEdges(DirectedEdgeStar.java:225)\n\tat org.locationtech.jts.geomgraph.PlanarGraph.linkResultDirectedEdges(PlanarGraph.java:61)\n\tat org.locationtech.jts.operation.overlay.PolygonBuilder.add(PolygonBuilder.java:66)\n\tat org.locationtech.jts.operation.overlay.PolygonBuilder.add(PolygonBuilder.java:56)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.computeOverlay(OverlayOp.java:248)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.getResultGeometry(OverlayOp.java:181)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.overlayOp(OverlayOp.java:84)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.getResultGeometry(SnapIfNeededOverlayOp.java:75)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.overlayOp(SnapIfNeededOverlayOp.java:37)\n\tat org.locationtech.jts.geom.GeometryOverlay.overlay(GeometryOverlay.java:76)\n\tat org.locationtech.jts.geom.GeometryOverlay.intersection(GeometryOverlay.java:119)\n\tat org.locationtech.jts.geom.Geometry.intersection(Geometry.java:1330)\n\tat org.apache.sedona.common.Functions.intersection(Functions.java:987)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:58)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:121)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression ST_Intersection - inputs: [POLYGON ((315056.626 5793716.89, 315053.766 5793719.782, 315058.583 5793724.51, 315058.044 5793725.017, 315061.31 5793728.339, 315061.411 5793728.25, 315063.105 5793729.971, 315060.495 5793732.497, 315061.296 5793733.282, 315056.48 5793738.006, 315063.126 5793744.858, 315073.89 5793734.467, 315066.974 5793727.383, 315064.59 5793725.022, 315061.447 5793721.812, 315060.804 5793721.166, 315056.626 5793716.89)), MULTIPOLYGON (((315050.07 5793723.45, 315056.63 5793716.89, 315053.75 5793719.77, 315050.07 5793723.45)), ((315050.07 5793723.45, 315053.75 5793719.77, 315056.63 5793716.89, 315050.52 5793714.47, 315045.44 5793708.28, 315031.32 5793688.73, 315024.03 5793672.83, 315015.58 5793655.35, 315000.21 5793624.54, 314993.26 5793610.15, 314988.86 5793601.52, 314985.81 5793595.53, 314979.26 5793598.82, 314969.12 5793602.4, 314960.37 5793605.91, 314955.01 5793608.05, 314949.88 5793609.89, 314944.57 5793609.55, 314939.79 5793609.23, 314933 5793606.27, 314928.72 5793604.86, 314916.03 5793616.9, 314903.12 5793629.64, 314925.5 5793653.58, 315006.84 5793720.15, 315031.7 5793738.82, 315034.07 5793740.61, 315050.35 5793755.49, 315059.64 5793747.79, 315042.87 5793730.47, 315050.07 5793723.45)))], cause: no outgoing dirEdge found [ (315056.2685, 5793717.251499999, NaN) ]\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:127)\n\tat org.apache.spark.sql.sedona_sql.expressions.implicits$InputExpressionEnhancer.toGeometry(implicits.scala:35)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredTypes$.$anonfun$buildArgumentExtractor$2(InferredExpression.scala:202)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.$anonfun$buildExtractors$2(InferredExpression.scala:97)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction1$2(InferrableFunctionConverter.scala:37)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5(TraitJoinQueryExec.scala:140)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5$adapted(TraitJoinQueryExec.scala:140)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.locationtech.jts.geom.TopologyException: no outgoing dirEdge found [ (315056.2685, 5793717.251499999, NaN) ]\n\tat org.locationtech.jts.geomgraph.DirectedEdgeStar.linkResultDirectedEdges(DirectedEdgeStar.java:225)\n\tat org.locationtech.jts.geomgraph.PlanarGraph.linkResultDirectedEdges(PlanarGraph.java:61)\n\tat org.locationtech.jts.operation.overlay.PolygonBuilder.add(PolygonBuilder.java:66)\n\tat org.locationtech.jts.operation.overlay.PolygonBuilder.add(PolygonBuilder.java:56)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.computeOverlay(OverlayOp.java:248)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.getResultGeometry(OverlayOp.java:181)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.overlayOp(OverlayOp.java:84)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.getResultGeometry(SnapIfNeededOverlayOp.java:75)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.overlayOp(SnapIfNeededOverlayOp.java:37)\n\tat org.locationtech.jts.geom.GeometryOverlay.overlay(GeometryOverlay.java:76)\n\tat org.locationtech.jts.geom.GeometryOverlay.intersection(GeometryOverlay.java:119)\n\tat org.locationtech.jts.geom.Geometry.intersection(Geometry.java:1330)\n\tat org.apache.sedona.common.Functions.intersection(Functions.java:987)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:58)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:121)\n\t... 26 more\n"
     ]
    }
   ],
   "source": [
    "result2_gdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "854de751-9a25-400c-b066-070e9c186ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result2_gdf = sedona.sql(\"\"\"\n",
    "                        SELECT building_oid\n",
    "                        FROM result\n",
    "                        WHERE ST_IsValid(geometry, 1) = false;\n",
    "                         \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "22764e70-8218-45a0-8222-6520d3203216",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/07 19:47:31 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "24/10/07 19:47:35 ERROR Executor: Exception in task 0.0 in stage 272.0 (TID 3284)\n",
      "org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression ST_Intersection - inputs: [POLYGON ((354735.34 5758459.87, 354741.52 5758459.29, 354741.47 5758458.76, 354742.003 5758458.661, 354742.51 5758458.467, 354742.974 5758458.186, 354743.379 5758457.825, 354743.713 5758457.397, 354743.964 5758456.917, 354744.125 5758456.399, 354744.19 5758455.86, 354744.839 5758455.842, 354744.839 5758454.833, 354744.11 5758454.92, 354743.905 5758454.454, 354743.625 5758454.029, 354743.276 5758453.658, 354742.87 5758453.351, 354742.418 5758453.117, 354741.933 5758452.963, 354741.429 5758452.894, 354740.92 5758452.91, 354740.87 5758452.33, 354734.69 5758452.91, 354734.47 5758450.53, 354719.262 5758452.06, 354719.872 5758458.208, 354718.444 5758458.366, 354719.11 5758464.36, 354735.59 5758462.51, 354735.34 5758459.87)), MULTIPOLYGON (((354742.48361941514 5758458.483279419, 354742.5175583665 5758458.466906712, 354742.5512882294 5758458.450107478, 354742.5848036437 5758458.432884384, 354742.5599997879 5758458.445518499, 354742.53506431903 5758458.457890828, 354742.51 5758458.47, 354742.50122503354 5758458.474463283, 354742.49243145384 5758458.478889782, 354742.48361941514 5758458.483279419)), ((354744.19 5758455.86, 354744.18363257905 5758455.978846373, 354744.173464127 5758456.097428019, 354744.15950506565 5758456.215623404, 354744.1743896288 5758456.097518557, 354744.18455998343 5758455.978914724, 354744.19 5758455.86)), ((354742.76916934864 5758458.3281861525, 354742.856506923 5758458.272293335, 354742.9418533804 5758458.213404956, 354743.02510602196 5758458.151591878, 354743.006871307 5758458.164586073, 354742.988501966 5758458.177389245, 354742.97 5758458.19, 354742.90448312287 5758458.238112903, 354742.83751848264 5758458.284189465, 354742.76916934864 5758458.3281861525)), ((354742.4807933091 5758453.143215798, 354742.36904797406 5758453.096683949, 354742.255514648 5758453.054702802, 354742.1403785658 5758453.01734085, 354742.23469866544 5758453.048476773, 354742.32793967956 5758453.082708925, 354742.42 5758453.12, 354742.44034595153 5758453.127524007, 354742.4606111444 5758453.135262895, 354742.4807933091 5758453.143215798)), ((354743.56067013886 5758453.949935026, 354743.47363280016 5758453.851548408, 354743.382367441 5758453.757070585, 354743.28704942006 5758453.666683088, 354743.380944933 5758453.758478318, 354743.4721767185 5758453.852921386, 354743.56067013886 5758453.949935026)), ((354741.50910225126 5758452.898420274, 354741.6329594108 5758452.913596253, 354741.75649912434 5758452.931170253, 354741.87967496796 5758452.95113567, 354741.7569187535 5758452.928421285, 354741.633323018 5758452.9108393155, 354741.50910225126 5758452.898420274)), ((354742.87 5758453.35, 354743.00843700604 5758453.446922913, 354743.143314634 5758453.548740796, 354743.2744598884 5758453.655323056, 354743.1454582713 5758453.545990117, 354743.01049506286 5758453.444107629, 354742.87 5758453.35), (354741.47 5758458.76, 354741.69640811626 5758458.730853419, 354741.9199317863 5758458.6845141575, 354742.1392674122 5758458.621252467, 354742.09311297646 5758458.635119098, 354742.04668400256 5758458.648036752, 354742 5758458.66, 354741.91386881284 5758458.682274899, 354741.8271822026 5758458.702279502, 354741.74 5758458.72, 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728, 354741.47 5758458.76)), ((354743.91 5758454.45, 354743.82046237413 5758454.307324911, 354743.7278918259 5758454.166904364, 354743.63 5758454.03, 354743.676666975 5758454.1000003815, 354743.7233338356 5758454.170000792, 354743.77 5758454.24, 354743.8166666031 5758454.309999824, 354743.8633327484 5758454.37999928, 354743.91 5758454.45), (354743.5687541657 5758457.608473395, 354743.7185425014 5758457.395727332, 354743.849262841 5758457.170761499, 354743.959924779 5758456.935280356, 354744.04968988255 5758456.691068032, 354744.0221770005 5758456.768298324, 354743.9922706891 5758456.844633698, 354743.96 5758456.92, 354743.9258946364 5758457.001157474, 354743.8892155034 5758457.081184673, 354743.85 5758457.16, 354743.8060313074 5758457.241553628, 354743.75934706524 5758457.321583757, 354743.71 5758457.4, 354743.6652958208 5758457.4710824955, 354743.6181956441 5758457.540600536, 354743.5687541657 5758457.608473395)))], cause: found non-noded intersection between LINESTRING ( 354741.47 5758458.76, 354741.69640811626 5758458.730853419 ) and LINESTRING ( 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728 ) [ (354741.5973395641, 5758458.743606979, NaN) ]\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:127)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.implicits$InputExpressionEnhancer.toGeometry(implicits.scala:35)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredTypes$.$anonfun$buildArgumentExtractor$2(InferredExpression.scala:202)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.$anonfun$buildExtractors$2(InferredExpression.scala:97)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:55)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n",
      "\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5(TraitJoinQueryExec.scala:140)\n",
      "\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5$adapted(TraitJoinQueryExec.scala:140)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: org.locationtech.jts.geom.TopologyException: found non-noded intersection between LINESTRING ( 354741.47 5758458.76, 354741.69640811626 5758458.730853419 ) and LINESTRING ( 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728 ) [ (354741.5973395641, 5758458.743606979, NaN) ]\n",
      "\tat org.locationtech.jts.noding.FastNodingValidator.checkValid(FastNodingValidator.java:139)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeNodingValidator.checkValid(EdgeNodingValidator.java:80)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeNodingValidator.checkValid(EdgeNodingValidator.java:45)\n",
      "\tat org.locationtech.jts.operation.overlay.OverlayOp.computeOverlay(OverlayOp.java:229)\n",
      "\tat org.locationtech.jts.operation.overlay.OverlayOp.getResultGeometry(OverlayOp.java:181)\n",
      "\tat org.locationtech.jts.operation.overlay.OverlayOp.overlayOp(OverlayOp.java:84)\n",
      "\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.getResultGeometry(SnapIfNeededOverlayOp.java:75)\n",
      "\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.overlayOp(SnapIfNeededOverlayOp.java:37)\n",
      "\tat org.locationtech.jts.geom.GeometryOverlay.overlay(GeometryOverlay.java:76)\n",
      "\tat org.locationtech.jts.geom.GeometryOverlay.intersection(GeometryOverlay.java:119)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersection(Geometry.java:1330)\n",
      "\tat org.apache.sedona.common.Functions.intersection(Functions.java:987)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:58)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:121)\n",
      "\t... 27 more\n",
      "24/10/07 19:47:35 WARN TaskSetManager: Lost task 0.0 in stage 272.0 (TID 3284) (28dd096dafc7 executor driver): org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression ST_Intersection - inputs: [POLYGON ((354735.34 5758459.87, 354741.52 5758459.29, 354741.47 5758458.76, 354742.003 5758458.661, 354742.51 5758458.467, 354742.974 5758458.186, 354743.379 5758457.825, 354743.713 5758457.397, 354743.964 5758456.917, 354744.125 5758456.399, 354744.19 5758455.86, 354744.839 5758455.842, 354744.839 5758454.833, 354744.11 5758454.92, 354743.905 5758454.454, 354743.625 5758454.029, 354743.276 5758453.658, 354742.87 5758453.351, 354742.418 5758453.117, 354741.933 5758452.963, 354741.429 5758452.894, 354740.92 5758452.91, 354740.87 5758452.33, 354734.69 5758452.91, 354734.47 5758450.53, 354719.262 5758452.06, 354719.872 5758458.208, 354718.444 5758458.366, 354719.11 5758464.36, 354735.59 5758462.51, 354735.34 5758459.87)), MULTIPOLYGON (((354742.48361941514 5758458.483279419, 354742.5175583665 5758458.466906712, 354742.5512882294 5758458.450107478, 354742.5848036437 5758458.432884384, 354742.5599997879 5758458.445518499, 354742.53506431903 5758458.457890828, 354742.51 5758458.47, 354742.50122503354 5758458.474463283, 354742.49243145384 5758458.478889782, 354742.48361941514 5758458.483279419)), ((354744.19 5758455.86, 354744.18363257905 5758455.978846373, 354744.173464127 5758456.097428019, 354744.15950506565 5758456.215623404, 354744.1743896288 5758456.097518557, 354744.18455998343 5758455.978914724, 354744.19 5758455.86)), ((354742.76916934864 5758458.3281861525, 354742.856506923 5758458.272293335, 354742.9418533804 5758458.213404956, 354743.02510602196 5758458.151591878, 354743.006871307 5758458.164586073, 354742.988501966 5758458.177389245, 354742.97 5758458.19, 354742.90448312287 5758458.238112903, 354742.83751848264 5758458.284189465, 354742.76916934864 5758458.3281861525)), ((354742.4807933091 5758453.143215798, 354742.36904797406 5758453.096683949, 354742.255514648 5758453.054702802, 354742.1403785658 5758453.01734085, 354742.23469866544 5758453.048476773, 354742.32793967956 5758453.082708925, 354742.42 5758453.12, 354742.44034595153 5758453.127524007, 354742.4606111444 5758453.135262895, 354742.4807933091 5758453.143215798)), ((354743.56067013886 5758453.949935026, 354743.47363280016 5758453.851548408, 354743.382367441 5758453.757070585, 354743.28704942006 5758453.666683088, 354743.380944933 5758453.758478318, 354743.4721767185 5758453.852921386, 354743.56067013886 5758453.949935026)), ((354741.50910225126 5758452.898420274, 354741.6329594108 5758452.913596253, 354741.75649912434 5758452.931170253, 354741.87967496796 5758452.95113567, 354741.7569187535 5758452.928421285, 354741.633323018 5758452.9108393155, 354741.50910225126 5758452.898420274)), ((354742.87 5758453.35, 354743.00843700604 5758453.446922913, 354743.143314634 5758453.548740796, 354743.2744598884 5758453.655323056, 354743.1454582713 5758453.545990117, 354743.01049506286 5758453.444107629, 354742.87 5758453.35), (354741.47 5758458.76, 354741.69640811626 5758458.730853419, 354741.9199317863 5758458.6845141575, 354742.1392674122 5758458.621252467, 354742.09311297646 5758458.635119098, 354742.04668400256 5758458.648036752, 354742 5758458.66, 354741.91386881284 5758458.682274899, 354741.8271822026 5758458.702279502, 354741.74 5758458.72, 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728, 354741.47 5758458.76)), ((354743.91 5758454.45, 354743.82046237413 5758454.307324911, 354743.7278918259 5758454.166904364, 354743.63 5758454.03, 354743.676666975 5758454.1000003815, 354743.7233338356 5758454.170000792, 354743.77 5758454.24, 354743.8166666031 5758454.309999824, 354743.8633327484 5758454.37999928, 354743.91 5758454.45), (354743.5687541657 5758457.608473395, 354743.7185425014 5758457.395727332, 354743.849262841 5758457.170761499, 354743.959924779 5758456.935280356, 354744.04968988255 5758456.691068032, 354744.0221770005 5758456.768298324, 354743.9922706891 5758456.844633698, 354743.96 5758456.92, 354743.9258946364 5758457.001157474, 354743.8892155034 5758457.081184673, 354743.85 5758457.16, 354743.8060313074 5758457.241553628, 354743.75934706524 5758457.321583757, 354743.71 5758457.4, 354743.6652958208 5758457.4710824955, 354743.6181956441 5758457.540600536, 354743.5687541657 5758457.608473395)))], cause: found non-noded intersection between LINESTRING ( 354741.47 5758458.76, 354741.69640811626 5758458.730853419 ) and LINESTRING ( 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728 ) [ (354741.5973395641, 5758458.743606979, NaN) ]\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:127)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.implicits$InputExpressionEnhancer.toGeometry(implicits.scala:35)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredTypes$.$anonfun$buildArgumentExtractor$2(InferredExpression.scala:202)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.$anonfun$buildExtractors$2(InferredExpression.scala:97)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:55)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n",
      "\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5(TraitJoinQueryExec.scala:140)\n",
      "\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5$adapted(TraitJoinQueryExec.scala:140)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: org.locationtech.jts.geom.TopologyException: found non-noded intersection between LINESTRING ( 354741.47 5758458.76, 354741.69640811626 5758458.730853419 ) and LINESTRING ( 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728 ) [ (354741.5973395641, 5758458.743606979, NaN) ]\n",
      "\tat org.locationtech.jts.noding.FastNodingValidator.checkValid(FastNodingValidator.java:139)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeNodingValidator.checkValid(EdgeNodingValidator.java:80)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeNodingValidator.checkValid(EdgeNodingValidator.java:45)\n",
      "\tat org.locationtech.jts.operation.overlay.OverlayOp.computeOverlay(OverlayOp.java:229)\n",
      "\tat org.locationtech.jts.operation.overlay.OverlayOp.getResultGeometry(OverlayOp.java:181)\n",
      "\tat org.locationtech.jts.operation.overlay.OverlayOp.overlayOp(OverlayOp.java:84)\n",
      "\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.getResultGeometry(SnapIfNeededOverlayOp.java:75)\n",
      "\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.overlayOp(SnapIfNeededOverlayOp.java:37)\n",
      "\tat org.locationtech.jts.geom.GeometryOverlay.overlay(GeometryOverlay.java:76)\n",
      "\tat org.locationtech.jts.geom.GeometryOverlay.intersection(GeometryOverlay.java:119)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersection(Geometry.java:1330)\n",
      "\tat org.apache.sedona.common.Functions.intersection(Functions.java:987)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:58)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:121)\n",
      "\t... 27 more\n",
      "\n",
      "24/10/07 19:47:35 ERROR TaskSetManager: Task 0 in stage 272.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o242.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 272.0 failed 1 times, most recent failure: Lost task 0.0 in stage 272.0 (TID 3284) (28dd096dafc7 executor driver): org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression ST_Intersection - inputs: [POLYGON ((354735.34 5758459.87, 354741.52 5758459.29, 354741.47 5758458.76, 354742.003 5758458.661, 354742.51 5758458.467, 354742.974 5758458.186, 354743.379 5758457.825, 354743.713 5758457.397, 354743.964 5758456.917, 354744.125 5758456.399, 354744.19 5758455.86, 354744.839 5758455.842, 354744.839 5758454.833, 354744.11 5758454.92, 354743.905 5758454.454, 354743.625 5758454.029, 354743.276 5758453.658, 354742.87 5758453.351, 354742.418 5758453.117, 354741.933 5758452.963, 354741.429 5758452.894, 354740.92 5758452.91, 354740.87 5758452.33, 354734.69 5758452.91, 354734.47 5758450.53, 354719.262 5758452.06, 354719.872 5758458.208, 354718.444 5758458.366, 354719.11 5758464.36, 354735.59 5758462.51, 354735.34 5758459.87)), MULTIPOLYGON (((354742.48361941514 5758458.483279419, 354742.5175583665 5758458.466906712, 354742.5512882294 5758458.450107478, 354742.5848036437 5758458.432884384, 354742.5599997879 5758458.445518499, 354742.53506431903 5758458.457890828, 354742.51 5758458.47, 354742.50122503354 5758458.474463283, 354742.49243145384 5758458.478889782, 354742.48361941514 5758458.483279419)), ((354744.19 5758455.86, 354744.18363257905 5758455.978846373, 354744.173464127 5758456.097428019, 354744.15950506565 5758456.215623404, 354744.1743896288 5758456.097518557, 354744.18455998343 5758455.978914724, 354744.19 5758455.86)), ((354742.76916934864 5758458.3281861525, 354742.856506923 5758458.272293335, 354742.9418533804 5758458.213404956, 354743.02510602196 5758458.151591878, 354743.006871307 5758458.164586073, 354742.988501966 5758458.177389245, 354742.97 5758458.19, 354742.90448312287 5758458.238112903, 354742.83751848264 5758458.284189465, 354742.76916934864 5758458.3281861525)), ((354742.4807933091 5758453.143215798, 354742.36904797406 5758453.096683949, 354742.255514648 5758453.054702802, 354742.1403785658 5758453.01734085, 354742.23469866544 5758453.048476773, 354742.32793967956 5758453.082708925, 354742.42 5758453.12, 354742.44034595153 5758453.127524007, 354742.4606111444 5758453.135262895, 354742.4807933091 5758453.143215798)), ((354743.56067013886 5758453.949935026, 354743.47363280016 5758453.851548408, 354743.382367441 5758453.757070585, 354743.28704942006 5758453.666683088, 354743.380944933 5758453.758478318, 354743.4721767185 5758453.852921386, 354743.56067013886 5758453.949935026)), ((354741.50910225126 5758452.898420274, 354741.6329594108 5758452.913596253, 354741.75649912434 5758452.931170253, 354741.87967496796 5758452.95113567, 354741.7569187535 5758452.928421285, 354741.633323018 5758452.9108393155, 354741.50910225126 5758452.898420274)), ((354742.87 5758453.35, 354743.00843700604 5758453.446922913, 354743.143314634 5758453.548740796, 354743.2744598884 5758453.655323056, 354743.1454582713 5758453.545990117, 354743.01049506286 5758453.444107629, 354742.87 5758453.35), (354741.47 5758458.76, 354741.69640811626 5758458.730853419, 354741.9199317863 5758458.6845141575, 354742.1392674122 5758458.621252467, 354742.09311297646 5758458.635119098, 354742.04668400256 5758458.648036752, 354742 5758458.66, 354741.91386881284 5758458.682274899, 354741.8271822026 5758458.702279502, 354741.74 5758458.72, 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728, 354741.47 5758458.76)), ((354743.91 5758454.45, 354743.82046237413 5758454.307324911, 354743.7278918259 5758454.166904364, 354743.63 5758454.03, 354743.676666975 5758454.1000003815, 354743.7233338356 5758454.170000792, 354743.77 5758454.24, 354743.8166666031 5758454.309999824, 354743.8633327484 5758454.37999928, 354743.91 5758454.45), (354743.5687541657 5758457.608473395, 354743.7185425014 5758457.395727332, 354743.849262841 5758457.170761499, 354743.959924779 5758456.935280356, 354744.04968988255 5758456.691068032, 354744.0221770005 5758456.768298324, 354743.9922706891 5758456.844633698, 354743.96 5758456.92, 354743.9258946364 5758457.001157474, 354743.8892155034 5758457.081184673, 354743.85 5758457.16, 354743.8060313074 5758457.241553628, 354743.75934706524 5758457.321583757, 354743.71 5758457.4, 354743.6652958208 5758457.4710824955, 354743.6181956441 5758457.540600536, 354743.5687541657 5758457.608473395)))], cause: found non-noded intersection between LINESTRING ( 354741.47 5758458.76, 354741.69640811626 5758458.730853419 ) and LINESTRING ( 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728 ) [ (354741.5973395641, 5758458.743606979, NaN) ]\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:127)\n\tat org.apache.spark.sql.sedona_sql.expressions.implicits$InputExpressionEnhancer.toGeometry(implicits.scala:35)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredTypes$.$anonfun$buildArgumentExtractor$2(InferredExpression.scala:202)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.$anonfun$buildExtractors$2(InferredExpression.scala:97)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:55)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5(TraitJoinQueryExec.scala:140)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5$adapted(TraitJoinQueryExec.scala:140)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.locationtech.jts.geom.TopologyException: found non-noded intersection between LINESTRING ( 354741.47 5758458.76, 354741.69640811626 5758458.730853419 ) and LINESTRING ( 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728 ) [ (354741.5973395641, 5758458.743606979, NaN) ]\n\tat org.locationtech.jts.noding.FastNodingValidator.checkValid(FastNodingValidator.java:139)\n\tat org.locationtech.jts.geomgraph.EdgeNodingValidator.checkValid(EdgeNodingValidator.java:80)\n\tat org.locationtech.jts.geomgraph.EdgeNodingValidator.checkValid(EdgeNodingValidator.java:45)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.computeOverlay(OverlayOp.java:229)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.getResultGeometry(OverlayOp.java:181)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.overlayOp(OverlayOp.java:84)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.getResultGeometry(SnapIfNeededOverlayOp.java:75)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.overlayOp(SnapIfNeededOverlayOp.java:37)\n\tat org.locationtech.jts.geom.GeometryOverlay.overlay(GeometryOverlay.java:76)\n\tat org.locationtech.jts.geom.GeometryOverlay.intersection(GeometryOverlay.java:119)\n\tat org.locationtech.jts.geom.Geometry.intersection(Geometry.java:1330)\n\tat org.apache.sedona.common.Functions.intersection(Functions.java:987)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:58)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:121)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression ST_Intersection - inputs: [POLYGON ((354735.34 5758459.87, 354741.52 5758459.29, 354741.47 5758458.76, 354742.003 5758458.661, 354742.51 5758458.467, 354742.974 5758458.186, 354743.379 5758457.825, 354743.713 5758457.397, 354743.964 5758456.917, 354744.125 5758456.399, 354744.19 5758455.86, 354744.839 5758455.842, 354744.839 5758454.833, 354744.11 5758454.92, 354743.905 5758454.454, 354743.625 5758454.029, 354743.276 5758453.658, 354742.87 5758453.351, 354742.418 5758453.117, 354741.933 5758452.963, 354741.429 5758452.894, 354740.92 5758452.91, 354740.87 5758452.33, 354734.69 5758452.91, 354734.47 5758450.53, 354719.262 5758452.06, 354719.872 5758458.208, 354718.444 5758458.366, 354719.11 5758464.36, 354735.59 5758462.51, 354735.34 5758459.87)), MULTIPOLYGON (((354742.48361941514 5758458.483279419, 354742.5175583665 5758458.466906712, 354742.5512882294 5758458.450107478, 354742.5848036437 5758458.432884384, 354742.5599997879 5758458.445518499, 354742.53506431903 5758458.457890828, 354742.51 5758458.47, 354742.50122503354 5758458.474463283, 354742.49243145384 5758458.478889782, 354742.48361941514 5758458.483279419)), ((354744.19 5758455.86, 354744.18363257905 5758455.978846373, 354744.173464127 5758456.097428019, 354744.15950506565 5758456.215623404, 354744.1743896288 5758456.097518557, 354744.18455998343 5758455.978914724, 354744.19 5758455.86)), ((354742.76916934864 5758458.3281861525, 354742.856506923 5758458.272293335, 354742.9418533804 5758458.213404956, 354743.02510602196 5758458.151591878, 354743.006871307 5758458.164586073, 354742.988501966 5758458.177389245, 354742.97 5758458.19, 354742.90448312287 5758458.238112903, 354742.83751848264 5758458.284189465, 354742.76916934864 5758458.3281861525)), ((354742.4807933091 5758453.143215798, 354742.36904797406 5758453.096683949, 354742.255514648 5758453.054702802, 354742.1403785658 5758453.01734085, 354742.23469866544 5758453.048476773, 354742.32793967956 5758453.082708925, 354742.42 5758453.12, 354742.44034595153 5758453.127524007, 354742.4606111444 5758453.135262895, 354742.4807933091 5758453.143215798)), ((354743.56067013886 5758453.949935026, 354743.47363280016 5758453.851548408, 354743.382367441 5758453.757070585, 354743.28704942006 5758453.666683088, 354743.380944933 5758453.758478318, 354743.4721767185 5758453.852921386, 354743.56067013886 5758453.949935026)), ((354741.50910225126 5758452.898420274, 354741.6329594108 5758452.913596253, 354741.75649912434 5758452.931170253, 354741.87967496796 5758452.95113567, 354741.7569187535 5758452.928421285, 354741.633323018 5758452.9108393155, 354741.50910225126 5758452.898420274)), ((354742.87 5758453.35, 354743.00843700604 5758453.446922913, 354743.143314634 5758453.548740796, 354743.2744598884 5758453.655323056, 354743.1454582713 5758453.545990117, 354743.01049506286 5758453.444107629, 354742.87 5758453.35), (354741.47 5758458.76, 354741.69640811626 5758458.730853419, 354741.9199317863 5758458.6845141575, 354742.1392674122 5758458.621252467, 354742.09311297646 5758458.635119098, 354742.04668400256 5758458.648036752, 354742 5758458.66, 354741.91386881284 5758458.682274899, 354741.8271822026 5758458.702279502, 354741.74 5758458.72, 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728, 354741.47 5758458.76)), ((354743.91 5758454.45, 354743.82046237413 5758454.307324911, 354743.7278918259 5758454.166904364, 354743.63 5758454.03, 354743.676666975 5758454.1000003815, 354743.7233338356 5758454.170000792, 354743.77 5758454.24, 354743.8166666031 5758454.309999824, 354743.8633327484 5758454.37999928, 354743.91 5758454.45), (354743.5687541657 5758457.608473395, 354743.7185425014 5758457.395727332, 354743.849262841 5758457.170761499, 354743.959924779 5758456.935280356, 354744.04968988255 5758456.691068032, 354744.0221770005 5758456.768298324, 354743.9922706891 5758456.844633698, 354743.96 5758456.92, 354743.9258946364 5758457.001157474, 354743.8892155034 5758457.081184673, 354743.85 5758457.16, 354743.8060313074 5758457.241553628, 354743.75934706524 5758457.321583757, 354743.71 5758457.4, 354743.6652958208 5758457.4710824955, 354743.6181956441 5758457.540600536, 354743.5687541657 5758457.608473395)))], cause: found non-noded intersection between LINESTRING ( 354741.47 5758458.76, 354741.69640811626 5758458.730853419 ) and LINESTRING ( 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728 ) [ (354741.5973395641, 5758458.743606979, NaN) ]\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:127)\n\tat org.apache.spark.sql.sedona_sql.expressions.implicits$InputExpressionEnhancer.toGeometry(implicits.scala:35)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredTypes$.$anonfun$buildArgumentExtractor$2(InferredExpression.scala:202)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.$anonfun$buildExtractors$2(InferredExpression.scala:97)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:55)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5(TraitJoinQueryExec.scala:140)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5$adapted(TraitJoinQueryExec.scala:140)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.locationtech.jts.geom.TopologyException: found non-noded intersection between LINESTRING ( 354741.47 5758458.76, 354741.69640811626 5758458.730853419 ) and LINESTRING ( 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728 ) [ (354741.5973395641, 5758458.743606979, NaN) ]\n\tat org.locationtech.jts.noding.FastNodingValidator.checkValid(FastNodingValidator.java:139)\n\tat org.locationtech.jts.geomgraph.EdgeNodingValidator.checkValid(EdgeNodingValidator.java:80)\n\tat org.locationtech.jts.geomgraph.EdgeNodingValidator.checkValid(EdgeNodingValidator.java:45)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.computeOverlay(OverlayOp.java:229)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.getResultGeometry(OverlayOp.java:181)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.overlayOp(OverlayOp.java:84)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.getResultGeometry(SnapIfNeededOverlayOp.java:75)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.overlayOp(SnapIfNeededOverlayOp.java:37)\n\tat org.locationtech.jts.geom.GeometryOverlay.overlay(GeometryOverlay.java:76)\n\tat org.locationtech.jts.geom.GeometryOverlay.intersection(GeometryOverlay.java:119)\n\tat org.locationtech.jts.geom.Geometry.intersection(Geometry.java:1330)\n\tat org.apache.sedona.common.Functions.intersection(Functions.java:987)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:58)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:121)\n\t... 27 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult2_gdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o242.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 272.0 failed 1 times, most recent failure: Lost task 0.0 in stage 272.0 (TID 3284) (28dd096dafc7 executor driver): org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression ST_Intersection - inputs: [POLYGON ((354735.34 5758459.87, 354741.52 5758459.29, 354741.47 5758458.76, 354742.003 5758458.661, 354742.51 5758458.467, 354742.974 5758458.186, 354743.379 5758457.825, 354743.713 5758457.397, 354743.964 5758456.917, 354744.125 5758456.399, 354744.19 5758455.86, 354744.839 5758455.842, 354744.839 5758454.833, 354744.11 5758454.92, 354743.905 5758454.454, 354743.625 5758454.029, 354743.276 5758453.658, 354742.87 5758453.351, 354742.418 5758453.117, 354741.933 5758452.963, 354741.429 5758452.894, 354740.92 5758452.91, 354740.87 5758452.33, 354734.69 5758452.91, 354734.47 5758450.53, 354719.262 5758452.06, 354719.872 5758458.208, 354718.444 5758458.366, 354719.11 5758464.36, 354735.59 5758462.51, 354735.34 5758459.87)), MULTIPOLYGON (((354742.48361941514 5758458.483279419, 354742.5175583665 5758458.466906712, 354742.5512882294 5758458.450107478, 354742.5848036437 5758458.432884384, 354742.5599997879 5758458.445518499, 354742.53506431903 5758458.457890828, 354742.51 5758458.47, 354742.50122503354 5758458.474463283, 354742.49243145384 5758458.478889782, 354742.48361941514 5758458.483279419)), ((354744.19 5758455.86, 354744.18363257905 5758455.978846373, 354744.173464127 5758456.097428019, 354744.15950506565 5758456.215623404, 354744.1743896288 5758456.097518557, 354744.18455998343 5758455.978914724, 354744.19 5758455.86)), ((354742.76916934864 5758458.3281861525, 354742.856506923 5758458.272293335, 354742.9418533804 5758458.213404956, 354743.02510602196 5758458.151591878, 354743.006871307 5758458.164586073, 354742.988501966 5758458.177389245, 354742.97 5758458.19, 354742.90448312287 5758458.238112903, 354742.83751848264 5758458.284189465, 354742.76916934864 5758458.3281861525)), ((354742.4807933091 5758453.143215798, 354742.36904797406 5758453.096683949, 354742.255514648 5758453.054702802, 354742.1403785658 5758453.01734085, 354742.23469866544 5758453.048476773, 354742.32793967956 5758453.082708925, 354742.42 5758453.12, 354742.44034595153 5758453.127524007, 354742.4606111444 5758453.135262895, 354742.4807933091 5758453.143215798)), ((354743.56067013886 5758453.949935026, 354743.47363280016 5758453.851548408, 354743.382367441 5758453.757070585, 354743.28704942006 5758453.666683088, 354743.380944933 5758453.758478318, 354743.4721767185 5758453.852921386, 354743.56067013886 5758453.949935026)), ((354741.50910225126 5758452.898420274, 354741.6329594108 5758452.913596253, 354741.75649912434 5758452.931170253, 354741.87967496796 5758452.95113567, 354741.7569187535 5758452.928421285, 354741.633323018 5758452.9108393155, 354741.50910225126 5758452.898420274)), ((354742.87 5758453.35, 354743.00843700604 5758453.446922913, 354743.143314634 5758453.548740796, 354743.2744598884 5758453.655323056, 354743.1454582713 5758453.545990117, 354743.01049506286 5758453.444107629, 354742.87 5758453.35), (354741.47 5758458.76, 354741.69640811626 5758458.730853419, 354741.9199317863 5758458.6845141575, 354742.1392674122 5758458.621252467, 354742.09311297646 5758458.635119098, 354742.04668400256 5758458.648036752, 354742 5758458.66, 354741.91386881284 5758458.682274899, 354741.8271822026 5758458.702279502, 354741.74 5758458.72, 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728, 354741.47 5758458.76)), ((354743.91 5758454.45, 354743.82046237413 5758454.307324911, 354743.7278918259 5758454.166904364, 354743.63 5758454.03, 354743.676666975 5758454.1000003815, 354743.7233338356 5758454.170000792, 354743.77 5758454.24, 354743.8166666031 5758454.309999824, 354743.8633327484 5758454.37999928, 354743.91 5758454.45), (354743.5687541657 5758457.608473395, 354743.7185425014 5758457.395727332, 354743.849262841 5758457.170761499, 354743.959924779 5758456.935280356, 354744.04968988255 5758456.691068032, 354744.0221770005 5758456.768298324, 354743.9922706891 5758456.844633698, 354743.96 5758456.92, 354743.9258946364 5758457.001157474, 354743.8892155034 5758457.081184673, 354743.85 5758457.16, 354743.8060313074 5758457.241553628, 354743.75934706524 5758457.321583757, 354743.71 5758457.4, 354743.6652958208 5758457.4710824955, 354743.6181956441 5758457.540600536, 354743.5687541657 5758457.608473395)))], cause: found non-noded intersection between LINESTRING ( 354741.47 5758458.76, 354741.69640811626 5758458.730853419 ) and LINESTRING ( 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728 ) [ (354741.5973395641, 5758458.743606979, NaN) ]\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:127)\n\tat org.apache.spark.sql.sedona_sql.expressions.implicits$InputExpressionEnhancer.toGeometry(implicits.scala:35)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredTypes$.$anonfun$buildArgumentExtractor$2(InferredExpression.scala:202)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.$anonfun$buildExtractors$2(InferredExpression.scala:97)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:55)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5(TraitJoinQueryExec.scala:140)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5$adapted(TraitJoinQueryExec.scala:140)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.locationtech.jts.geom.TopologyException: found non-noded intersection between LINESTRING ( 354741.47 5758458.76, 354741.69640811626 5758458.730853419 ) and LINESTRING ( 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728 ) [ (354741.5973395641, 5758458.743606979, NaN) ]\n\tat org.locationtech.jts.noding.FastNodingValidator.checkValid(FastNodingValidator.java:139)\n\tat org.locationtech.jts.geomgraph.EdgeNodingValidator.checkValid(EdgeNodingValidator.java:80)\n\tat org.locationtech.jts.geomgraph.EdgeNodingValidator.checkValid(EdgeNodingValidator.java:45)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.computeOverlay(OverlayOp.java:229)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.getResultGeometry(OverlayOp.java:181)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.overlayOp(OverlayOp.java:84)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.getResultGeometry(SnapIfNeededOverlayOp.java:75)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.overlayOp(SnapIfNeededOverlayOp.java:37)\n\tat org.locationtech.jts.geom.GeometryOverlay.overlay(GeometryOverlay.java:76)\n\tat org.locationtech.jts.geom.GeometryOverlay.intersection(GeometryOverlay.java:119)\n\tat org.locationtech.jts.geom.Geometry.intersection(Geometry.java:1330)\n\tat org.apache.sedona.common.Functions.intersection(Functions.java:987)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:58)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:121)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression ST_Intersection - inputs: [POLYGON ((354735.34 5758459.87, 354741.52 5758459.29, 354741.47 5758458.76, 354742.003 5758458.661, 354742.51 5758458.467, 354742.974 5758458.186, 354743.379 5758457.825, 354743.713 5758457.397, 354743.964 5758456.917, 354744.125 5758456.399, 354744.19 5758455.86, 354744.839 5758455.842, 354744.839 5758454.833, 354744.11 5758454.92, 354743.905 5758454.454, 354743.625 5758454.029, 354743.276 5758453.658, 354742.87 5758453.351, 354742.418 5758453.117, 354741.933 5758452.963, 354741.429 5758452.894, 354740.92 5758452.91, 354740.87 5758452.33, 354734.69 5758452.91, 354734.47 5758450.53, 354719.262 5758452.06, 354719.872 5758458.208, 354718.444 5758458.366, 354719.11 5758464.36, 354735.59 5758462.51, 354735.34 5758459.87)), MULTIPOLYGON (((354742.48361941514 5758458.483279419, 354742.5175583665 5758458.466906712, 354742.5512882294 5758458.450107478, 354742.5848036437 5758458.432884384, 354742.5599997879 5758458.445518499, 354742.53506431903 5758458.457890828, 354742.51 5758458.47, 354742.50122503354 5758458.474463283, 354742.49243145384 5758458.478889782, 354742.48361941514 5758458.483279419)), ((354744.19 5758455.86, 354744.18363257905 5758455.978846373, 354744.173464127 5758456.097428019, 354744.15950506565 5758456.215623404, 354744.1743896288 5758456.097518557, 354744.18455998343 5758455.978914724, 354744.19 5758455.86)), ((354742.76916934864 5758458.3281861525, 354742.856506923 5758458.272293335, 354742.9418533804 5758458.213404956, 354743.02510602196 5758458.151591878, 354743.006871307 5758458.164586073, 354742.988501966 5758458.177389245, 354742.97 5758458.19, 354742.90448312287 5758458.238112903, 354742.83751848264 5758458.284189465, 354742.76916934864 5758458.3281861525)), ((354742.4807933091 5758453.143215798, 354742.36904797406 5758453.096683949, 354742.255514648 5758453.054702802, 354742.1403785658 5758453.01734085, 354742.23469866544 5758453.048476773, 354742.32793967956 5758453.082708925, 354742.42 5758453.12, 354742.44034595153 5758453.127524007, 354742.4606111444 5758453.135262895, 354742.4807933091 5758453.143215798)), ((354743.56067013886 5758453.949935026, 354743.47363280016 5758453.851548408, 354743.382367441 5758453.757070585, 354743.28704942006 5758453.666683088, 354743.380944933 5758453.758478318, 354743.4721767185 5758453.852921386, 354743.56067013886 5758453.949935026)), ((354741.50910225126 5758452.898420274, 354741.6329594108 5758452.913596253, 354741.75649912434 5758452.931170253, 354741.87967496796 5758452.95113567, 354741.7569187535 5758452.928421285, 354741.633323018 5758452.9108393155, 354741.50910225126 5758452.898420274)), ((354742.87 5758453.35, 354743.00843700604 5758453.446922913, 354743.143314634 5758453.548740796, 354743.2744598884 5758453.655323056, 354743.1454582713 5758453.545990117, 354743.01049506286 5758453.444107629, 354742.87 5758453.35), (354741.47 5758458.76, 354741.69640811626 5758458.730853419, 354741.9199317863 5758458.6845141575, 354742.1392674122 5758458.621252467, 354742.09311297646 5758458.635119098, 354742.04668400256 5758458.648036752, 354742 5758458.66, 354741.91386881284 5758458.682274899, 354741.8271822026 5758458.702279502, 354741.74 5758458.72, 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728, 354741.47 5758458.76)), ((354743.91 5758454.45, 354743.82046237413 5758454.307324911, 354743.7278918259 5758454.166904364, 354743.63 5758454.03, 354743.676666975 5758454.1000003815, 354743.7233338356 5758454.170000792, 354743.77 5758454.24, 354743.8166666031 5758454.309999824, 354743.8633327484 5758454.37999928, 354743.91 5758454.45), (354743.5687541657 5758457.608473395, 354743.7185425014 5758457.395727332, 354743.849262841 5758457.170761499, 354743.959924779 5758456.935280356, 354744.04968988255 5758456.691068032, 354744.0221770005 5758456.768298324, 354743.9922706891 5758456.844633698, 354743.96 5758456.92, 354743.9258946364 5758457.001157474, 354743.8892155034 5758457.081184673, 354743.85 5758457.16, 354743.8060313074 5758457.241553628, 354743.75934706524 5758457.321583757, 354743.71 5758457.4, 354743.6652958208 5758457.4710824955, 354743.6181956441 5758457.540600536, 354743.5687541657 5758457.608473395)))], cause: found non-noded intersection between LINESTRING ( 354741.47 5758458.76, 354741.69640811626 5758458.730853419 ) and LINESTRING ( 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728 ) [ (354741.5973395641, 5758458.743606979, NaN) ]\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:127)\n\tat org.apache.spark.sql.sedona_sql.expressions.implicits$InputExpressionEnhancer.toGeometry(implicits.scala:35)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredTypes$.$anonfun$buildArgumentExtractor$2(InferredExpression.scala:202)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.$anonfun$buildExtractors$2(InferredExpression.scala:97)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:55)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5(TraitJoinQueryExec.scala:140)\n\tat org.apache.spark.sql.sedona_sql.strategy.join.TraitJoinQueryExec.$anonfun$doExecute$5$adapted(TraitJoinQueryExec.scala:140)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.locationtech.jts.geom.TopologyException: found non-noded intersection between LINESTRING ( 354741.47 5758458.76, 354741.69640811626 5758458.730853419 ) and LINESTRING ( 354741.6503692072 5758458.735750736, 354741.5603475359 5758458.74908728 ) [ (354741.5973395641, 5758458.743606979, NaN) ]\n\tat org.locationtech.jts.noding.FastNodingValidator.checkValid(FastNodingValidator.java:139)\n\tat org.locationtech.jts.geomgraph.EdgeNodingValidator.checkValid(EdgeNodingValidator.java:80)\n\tat org.locationtech.jts.geomgraph.EdgeNodingValidator.checkValid(EdgeNodingValidator.java:45)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.computeOverlay(OverlayOp.java:229)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.getResultGeometry(OverlayOp.java:181)\n\tat org.locationtech.jts.operation.overlay.OverlayOp.overlayOp(OverlayOp.java:84)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.getResultGeometry(SnapIfNeededOverlayOp.java:75)\n\tat org.locationtech.jts.operation.overlay.snap.SnapIfNeededOverlayOp.overlayOp(SnapIfNeededOverlayOp.java:37)\n\tat org.locationtech.jts.geom.GeometryOverlay.overlay(GeometryOverlay.java:76)\n\tat org.locationtech.jts.geom.GeometryOverlay.intersection(GeometryOverlay.java:119)\n\tat org.locationtech.jts.geom.Geometry.intersection(Geometry.java:1330)\n\tat org.apache.sedona.common.Functions.intersection(Functions.java:987)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Intersection$$anonfun$$lessinit$greater$28.apply(Functions.scala:296)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction2$2(InferrableFunctionConverter.scala:58)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.evalWithoutSerialization(InferredExpression.scala:121)\n\t... 27 more\n"
     ]
    }
   ],
   "source": [
    "result2_gdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "098113b8-9ae4-4804-8068-6801e44dd309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/07 19:28:51 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task                      (0 + 16) / 32]\n",
      "org.locationtech.jts.geom.TopologyException: side location conflict [ (404524.85, 5795432.82, NaN) ]\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:128)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n",
      "\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n",
      "\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:222)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 ERROR Executor: Exception in task 7.0 in stage 217.0 (TID 2686)\n",
      "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/app/geospeed/sedona_data.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: org.locationtech.jts.geom.TopologyException: side location conflict [ (404524.85, 5795432.82, NaN) ]\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:128)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n",
      "\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n",
      "\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:222)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\t... 15 more\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 7.0 in stage 217.0 (TID 2686) (28dd096dafc7 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/app/geospeed/sedona_data.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: org.locationtech.jts.geom.TopologyException: side location conflict [ (404524.85, 5795432.82, NaN) ]\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:128)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n",
      "\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n",
      "\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:222)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\t... 15 more\n",
      "\n",
      "24/10/07 19:28:57 ERROR TaskSetManager: Task 7 in stage 217.0 failed 1 times; aborting job\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Aborting job 5b88bf52-727d-46ee-ba6d-ebad1f1f1002.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 217.0 failed 1 times, most recent failure: Lost task 7.0 in stage 217.0 (TID 2686) (28dd096dafc7 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/app/geospeed/sedona_data.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: org.locationtech.jts.geom.TopologyException: side location conflict [ (404524.85, 5795432.82, NaN) ]\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:128)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n",
      "\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n",
      "\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:222)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\t... 15 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/app/geospeed/sedona_data.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n",
      "Caused by: org.locationtech.jts.geom.TopologyException: side location conflict [ (404524.85, 5795432.82, NaN) ]\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n",
      "\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:128)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n",
      "\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n",
      "\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n",
      "\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n",
      "\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n",
      "\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:222)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\t... 15 more\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:213)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:204)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:204)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:204)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:204)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:204)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:213)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:213)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:213)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:204)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:204)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:204)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:204)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 16.0 in stage 217.0 (TID 2695) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 WARN FileOutputCommitter: Could not delete file:/app/geospeed/sedona_data.parquet/_temporary/0/_temporary/attempt_202410071928519169595193606383901_0217_m_000001_2680\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 1.0 in stage 217.0 (TID 2680) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:204)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 WARN FileOutputCommitter: Could not delete file:/app/geospeed/sedona_data.parquet/_temporary/0/_temporary/attempt_202410071928519169595193606383901_0217_m_000003_2682\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:204)\n",
      "\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n",
      "\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n",
      "\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 3.0 in stage 217.0 (TID 2682) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 WARN FileOutputCommitter: Could not delete file:/app/geospeed/sedona_data.parquet/_temporary/0/_temporary/attempt_202410071928519169595193606383901_0217_m_000008_2687\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 8.0 in stage 217.0 (TID 2687) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 WARN FileOutputCommitter: Could not delete file:/app/geospeed/sedona_data.parquet/_temporary/0/_temporary/attempt_202410071928519169595193606383901_0217_m_000000_2679\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 0.0 in stage 217.0 (TID 2679) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 WARN FileOutputCommitter: Could not delete file:/app/geospeed/sedona_data.parquet/_temporary/0/_temporary/attempt_202410071928519169595193606383901_0217_m_000004_2683\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 4.0 in stage 217.0 (TID 2683) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 WARN FileOutputCommitter: Could not delete file:/app/geospeed/sedona_data.parquet/_temporary/0/_temporary/attempt_202410071928519169595193606383901_0217_m_000009_2688\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 WARN FileOutputCommitter: Could not delete file:/app/geospeed/sedona_data.parquet/_temporary/0/_temporary/attempt_202410071928519169595193606383901_0217_m_000005_2684\n",
      "24/10/07 19:28:57 WARN FileOutputCommitter: Could not delete file:/app/geospeed/sedona_data.parquet/_temporary/0/_temporary/attempt_202410071928519169595193606383901_0217_m_000002_2681\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 WARN FileOutputCommitter: Could not delete file:/app/geospeed/sedona_data.parquet/_temporary/0/_temporary/attempt_202410071928519169595193606383901_0217_m_000006_2685\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 9.0 in stage 217.0 (TID 2688) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 2.0 in stage 217.0 (TID 2681) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 6.0 in stage 217.0 (TID 2685) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 5.0 in stage 217.0 (TID 2684) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 WARN FileOutputCommitter: Could not delete file:/app/geospeed/sedona_data.parquet/_temporary/0/_temporary/attempt_202410071928519169595193606383901_0217_m_000010_2689\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 12.0 in stage 217.0 (TID 2691) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 10.0 in stage 217.0 (TID 2689) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 11.0 in stage 217.0 (TID 2690) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 13.0 in stage 217.0 (TID 2692) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 15.0 in stage 217.0 (TID 2694) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n",
      "24/10/07 19:28:57 ERROR FileFormatWriter: Job job_202410071928519169595193606383901_0217 aborted.\n",
      "24/10/07 19:28:57 WARN TaskSetManager: Lost task 14.0 in stage 217.0 (TID 2693) (28dd096dafc7 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o183.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 217.0 failed 1 times, most recent failure: Lost task 7.0 in stage 217.0 (TID 2686) (28dd096dafc7 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/app/geospeed/sedona_data.parquet.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.locationtech.jts.geom.TopologyException: side location conflict [ (404524.85, 5795432.82, NaN) ]\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:128)\n\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:222)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/app/geospeed/sedona_data.parquet.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.locationtech.jts.geom.TopologyException: side location conflict [ (404524.85, 5795432.82, NaN) ]\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:128)\n\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:222)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$schema\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://proj.org/schemas/v0.7/projjson.schema.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProjectedCRS\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETRS89 / UTM zone 33N\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_crs\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETRS89\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatum_ensemble\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuropean Terrestrial Reference System 1989 ensemble\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmembers\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: [\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuropean Terrestrial Reference Frame 1989\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuropean Terrestrial Reference Frame 1990\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuropean Terrestrial Reference Frame 1991\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuropean Terrestrial Reference Frame 1992\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuropean Terrestrial Reference Frame 1993\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuropean Terrestrial Reference Frame 1994\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuropean Terrestrial Reference Frame 1996\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuropean Terrestrial Reference Frame 1997\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuropean Terrestrial Reference Frame 2000\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuropean Terrestrial Reference Frame 2005\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuropean Terrestrial Reference Frame 2014\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}],\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mellipsoid\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGRS 1980\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemi_major_axis\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 6378137,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minverse_flattening\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 298.257222101},\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m},\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoordinate_system\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mellipsoidal\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: [\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeodetic latitude\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabbreviation\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLat\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirection\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorth\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdegree\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeodetic longitude\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabbreviation\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLon\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirection\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meast\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdegree\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}]},\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthority\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 4258}},\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversion\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTM zone 33N\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransverse Mercator\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthority\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 9807}},\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: [\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatitude of natural origin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 0,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdegree\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthority\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 8801}},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLongitude of natural origin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 15,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdegree\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthority\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 8802}},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScale factor at natural origin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 0.9996,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munity\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthority\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 8805}},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse easting\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 500000,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetre\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthority\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 8806}},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse northing\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 0,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetre\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthority\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 8807}}]},\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoordinate_system\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartesian\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: [\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEasting\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabbreviation\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirection\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meast\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetre\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m},\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNorthing\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabbreviation\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirection\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorth\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetre\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}]},\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscope\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngineering survey, topographic mapping.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEurope between 12°E and 18°E: Austria; Denmark - offshore and offshore; Germany - onshore and offshore; Norway including Svalbard - onshore and offshore.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msouth_latitude\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 46.4,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwest_longitude\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 12,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorth_latitude\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 84.42,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meast_longitude\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 18.01},\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthority\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: 25833}}\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m (\u001b[43mresult2_gdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeoparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeoparquet.version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeoparquet.crs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 8\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msedona_data.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:1398\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1398\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o183.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 217.0 failed 1 times, most recent failure: Lost task 7.0 in stage 217.0 (TID 2686) (28dd096dafc7 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/app/geospeed/sedona_data.parquet.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.locationtech.jts.geom.TopologyException: side location conflict [ (404524.85, 5795432.82, NaN) ]\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:128)\n\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:222)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/app/geospeed/sedona_data.parquet.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.locationtech.jts.geom.TopologyException: side location conflict [ (404524.85, 5795432.82, NaN) ]\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.propagateSideLabels(EdgeEndStar.java:289)\n\tat org.locationtech.jts.geomgraph.EdgeEndStar.computeLabelling(EdgeEndStar.java:128)\n\tat org.locationtech.jts.operation.relate.RelateComputer.labelNodeEdges(RelateComputer.java:325)\n\tat org.locationtech.jts.operation.relate.RelateComputer.computeIM(RelateComputer.java:125)\n\tat org.locationtech.jts.operation.relate.RelateOp.getIntersectionMatrix(RelateOp.java:109)\n\tat org.locationtech.jts.operation.relate.RelateOp.relate(RelateOp.java:54)\n\tat org.locationtech.jts.geom.Geometry.relate(Geometry.java:1035)\n\tat org.locationtech.jts.geom.Geometry.intersects(Geometry.java:767)\n\tat org.apache.sedona.core.spatialOperator.SpatialPredicateEvaluators$IntersectsEvaluator.eval(SpatialPredicateEvaluators.java:47)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.match(JudgementBase.java:94)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.populateNextBatch(JudgementBase.java:222)\n\tat org.apache.sedona.core.joinJudgement.JudgementBase.nextBase(JudgementBase.java:152)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:93)\n\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement$1.next(DynamicIndexLookupJudgement.java:85)\n\tat org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:184)\n\tat org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "proj = \"\"\"{\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\",\"type\": \"ProjectedCRS\",\"name\": \"ETRS89 / UTM zone 33N\",\"base_crs\": {\"name\": \"ETRS89\",\"datum_ensemble\": {\"name\": \"European Terrestrial Reference System 1989 ensemble\",\"members\": [{\"name\": \"European Terrestrial Reference Frame 1989\"},{\"name\": \"European Terrestrial Reference Frame 1990\"},{\"name\": \"European Terrestrial Reference Frame 1991\"},{\"name\": \"European Terrestrial Reference Frame 1992\"},{\"name\": \"European Terrestrial Reference Frame 1993\"},{\"name\": \"European Terrestrial Reference Frame 1994\"},{\"name\": \"European Terrestrial Reference Frame 1996\"},{\"name\": \"European Terrestrial Reference Frame 1997\"},{\"name\": \"European Terrestrial Reference Frame 2000\"},{\"name\": \"European Terrestrial Reference Frame 2005\"},{\"name\": \"European Terrestrial Reference Frame 2014\"}],\"ellipsoid\": {\"name\": \"GRS 1980\",\"semi_major_axis\": 6378137,\"inverse_flattening\": 298.257222101},\"accuracy\": \"0.1\"},\"coordinate_system\": {\"subtype\": \"ellipsoidal\",\"axis\": [{\"name\": \"Geodetic latitude\",\"abbreviation\": \"Lat\",\"direction\": \"north\",\"unit\": \"degree\"},{\"name\": \"Geodetic longitude\",\"abbreviation\": \"Lon\",\"direction\": \"east\",\"unit\": \"degree\"}]},\"id\": {\"authority\": \"EPSG\",\"code\": 4258}},\"conversion\": {\"name\": \"UTM zone 33N\",\"method\": {\"name\": \"Transverse Mercator\",\"id\": {\"authority\": \"EPSG\",\"code\": 9807}},\"parameters\": [{\"name\": \"Latitude of natural origin\",\"value\": 0,\"unit\": \"degree\",\"id\": {\"authority\": \"EPSG\",\"code\": 8801}},{\"name\": \"Longitude of natural origin\",\"value\": 15,\"unit\": \"degree\",\"id\": {\"authority\": \"EPSG\",\"code\": 8802}},{\"name\": \"Scale factor at natural origin\",\"value\": 0.9996,\"unit\": \"unity\",\"id\": {\"authority\": \"EPSG\",\"code\": 8805}},{\"name\": \"False easting\",\"value\": 500000,\"unit\": \"metre\",\"id\": {\"authority\": \"EPSG\",\"code\": 8806}},{\"name\": \"False northing\",\"value\": 0,\"unit\": \"metre\",\"id\": {\"authority\": \"EPSG\",\"code\": 8807}}]},\"coordinate_system\": {\"subtype\": \"Cartesian\",\"axis\": [{\"name\": \"Easting\",\"abbreviation\": \"E\",\"direction\": \"east\",\"unit\": \"metre\"},{\"name\": \"Northing\",\"abbreviation\": \"N\",\"direction\": \"north\",\"unit\": \"metre\"}]},\"scope\": \"Engineering survey, topographic mapping.\",\"area\": \"Europe between 12°E and 18°E: Austria; Denmark - offshore and offshore; Germany - onshore and offshore; Norway including Svalbard - onshore and offshore.\",\"bbox\": {\"south_latitude\": 46.4,\"west_longitude\": 12,\"north_latitude\": 84.42,\"east_longitude\": 18.01},\"id\": {\"authority\": \"EPSG\",\"code\": 25833}}\"\"\"\n",
    "\n",
    "(result2_gdf.write\n",
    " .format(\"geoparquet\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"geoparquet.version\", \"1.0.0\")\n",
    " .option(\"geoparquet.crs\", proj)\n",
    " .save(\"sedona_data.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e713cc7-0aed-414c-a3d8-b5e5b74bfb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://28dd096dafc7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbd44af58d0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4aadc4b6-e52c-4bc8-aca3-d92c335d5b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264bcff8-77df-4e88-bde9-42ae006d2448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
